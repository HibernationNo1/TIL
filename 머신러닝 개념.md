# 머신러닝 개념

- 퍼셉트론(뉴런이라고도 함)

  실제 뇌의 신경세포 뉴런의 동작을 모방한 알고리즘으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘이다. 

  각각의 입력값에는 각각의 가중치가 존재하고, 가중치가 클수록 해당 입력 값이 중요하다는 것을 의미한다.

  입력값과 가중치를 곱한 각각의 값의 전체 합이 퍼셉트론의 임계치를 넘으면 1을 출력한다. (그 외에는 0을 출력한다.)

  > 임계치(threshold): 퍼셉트론이 활성화 되기 위한 최소값
  >
  > 가중치(weight): 선형 경계의 방향성 또는 형태를 나타내는 값
  >
  > 바이어스(bias): 선형 경계의 절편을 나타내는 값. 직선의 경우는 y절편(x = 0)
  >
  > net 값: 입력값과 가중치를 곱한 각각의 값의 전체 합

- 단층 퍼셉트론(Single-layer Perceptron)

  여러 퍼셉트론이 하나의 층을 이루고 있는 구조로 입력 벡터를 두 부류로 구분하는 선형분류기이다. 이를 통해 AND, NAND, OR게이트를 구현할 수 있다.

  XOR게이트는 비선형 분류이기 때문에 구현을 할 수 없다.

- 다층 퍼셉트론(Multi-layer Perceptron)

  입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 둔 구조이다.

  AND, NAND, OR게이트를 조합하여 XOR게이트를 구현하는 것 처럼, 비선형 분류를 구현하기 위한 구조이다.

  > 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN) 이라고 하며, 학습을 시키는 신경망이 심층 신경망일 경우 이를 **딥러닝(Deep Learning)**이라고 한다.

  - **문제점 1. 비선형 분류 구현의 한계**

    Hidden layer도 무작정 쌓기만 한다고 해서 퍼셉트론을 선형분류기에서 비선형분류기로 바꿀 수 있는 것은 아니다.  선형 시스템이 아무리 깊어지더라도 f(ax+by)=af(x) + bf(y)의 성질에 의해 linear한 연산을 갖는 layer를 수십개 쌓아도 결국 이는 하나의 linear 연산으로 나타낼 수 있기 때문이다.

    **해결방법**: 활성화 함수를 사용한다.

    > 아래에서 설명

  - **문제점 2. 학습이 불가능**

    Oupput Layer의 Target value가 있는 것과 달리 중간의 Hidden Layer가 가지고 있는 node에는 Target Value가 존재하지 않아 은닉층의 오차(목표값과 실측값의 차이)를 구하기 난해하여 학습을 시킬 방법이 없었다.

    **해결 방법**: 학습 방법으로 델타 규칙(Delta rule)을 사용한다.

    >  **델타 규칙**이란 오차를 줄이기 위해 가중치를 체계적으로 변경하는 방법을 의미한다. 
    >
    > 방법은, 어떤 입력 노드가 출력 노드의 오차에 기여했다면, 두 노드의 연결 가중치는 해당 입력 노드의 출력과 출력 노드의 오차에 비례해 조절한다.
    >
    > 이런 델타 규칙을 output에서 input방향으로 진행하며 Weight를 재업데이트 하는 것을 **역전파(Back Propagation)**라고 한다.

  - **문제점 3. Overfitting**

    복잡한 문제를 해결하기 위해 Hidden Layer를 늘림으로 인해 학습이 트레이닝 셋에 너무 과최적화되어 실제 데이터를 집어 넣어 분류를 하려고 하면 정확도가 좀 떨어지는 것을 의미한다.

    **해결방법**: 여러가지가 있지만 Dropout이 딥러닝에서 대표적으로 사용됨

    > Dropout은 학습 과정에서 일부 뉴런을 사용하지 않는 형태로 만들어서 오버피팅을 방지할 수 있도록 만들어주는 Regularization 기법이다.

  - **문제점 4. Vanishing Gradient Problem(기울기값이 사라지는 문제)**

    복잡한 문제를 해결하기 위해 다수의 Hidden Layer가 있을 때, 역전파가 진행돼는 동안 각각의 노드에 Delta rule이 적용되고 오차가 거의 0인 뉴런에 의해 입력층 쪽으로갈수록 대부분의 노드에서 기울기가 0이되어 결국 gradient가 거의 완전히 사라지는 현상. 결국 입력층쪽 노드들은 기울기가 사라지므로 학습이 되지 않게 된다. 

    **해결방법**: 활성화 함수를 변경한다.

    > Vanishing Gradient Problem은 활성화 함수가 Sigmoid일때 발생하기 때문에 활성화 함수를 ReLU로 바꿔준다. 대부분의 input값에 대해 기울기가 0이 아니기 때문에 Vanishing Gradient Problem가 발생하지 않는다.

- 활성화 함수

  최종출력 신호를 다음 뉴런으로 보내줄지 말지 결정하는 역할을 한다.

  이를 통해 퍼셉트론의 입력값에 대한 출력값이 linear하게 나오지 않게 결정해주는 함수이다.

  종류

  - **Sigmoid**

    일정 값을 기준으로 0인지 1인지구분함으로써 분류하는 방식으로 binary classification(이항 분류)에 적절함 함수다.

    Vanishing Gradient Problem가 발생할 수 있다.

  - **Tanh**

    sigmoid가 갖고 있던 최적화 과정에서 느려지는 문제를 해결했다.

    Vanishing Gradient Problem가 발생할 수 있다.

  - **ReLU**

    0보다 작은 값이 나온 경우 0을 반환하고, 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수다.

  - **leaky ReLU**

    0보다 작은 값이 나온 경우 그 값에 0.01을 곱한 값을 반환하고, 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수다.

    leaky ReLU를 일반적으로 많이 쓰진 않지만 ReLU보다 학습이 더 잘 되긴 한다.

  - **Softmax**

    입력받은 값을 0~1사이의 값으로 모두 정규화하며 출력한다. 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다.

    분류하고자 하는 클래스가 여러개 일 때 확률을 표현하기 위해 사용한다.