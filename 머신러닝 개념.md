# 머신러닝 개념

### 개념

##### 1. 인공지능

인공지능이란 기계가 주어진 문제를 합리적인 사고를 통해 해결하는 인지적인 능력을 갖추고 있고 이러한 문제를 해결하는 경험을 통해 능력을 향상하는 학습 능력을 갖추도록 하는 기술 

---

##### 2. 머신러닝

머신러닝이란 데이터를 구문 분석하고 해당 데이터를 통해 학습한 후 정보를 바탕으로 결정을 내리기 위해 학습한 내용을 적용하는 알고리즘으로 인공지능의 기법 중 하나이다.

명시적인 지시 없이 학습할 수 있는 능력을 가진 기계나 장치를 만드는 방법이다. 머신러닝의 현상은 기계가 자동으로 학습하고 패턴을 식별하며 결정을 내릴 수 있게 한다. 

**머신러닝 종류**

- **귀납적 머신러닝**

  통계적 학습을 통해 기계학습을 하는 방법

  표본으로부터 일반화하는 학습 방법이다.

- **연역 머신러닝**

  논리적 학습을 통해 기계학습을 하는 방법

  (이게 가능한가..? 백준 알고리즘?)

**머신러닝 알고리즘 종류**

- **Naive Bayes**

  지도학습의 일종으로, 베이즈 정리를 기반으로 한 분류 알고리즘

- **Support Vector Machine**

  지도학습의 일종으로, 분류를 위한 기준 선을 정의해서 새로운 데이터를 분류하는 알고리즘

- **Decision Tree**

  지도학습의 일종으로, 데이터를 분석하여 소집단으로 분류하거나 데이터 사이에 존재하는 패턴을 예측하는 알고리즘

  분류(classification)와 회귀(regression) 모두 가능

  - 장점

    학습된 결과를 사람이 이해하기 쉬우며 누락된 값이 있어도 처리할 수 있다. 

    분류와 관련이 없는 속성이 있어도 처리할 수 있다.

  - 단점

    불안정성이 높다. 데이터의 특성이 특정 변수에 수직/수평적으로 구분되지 못할 때 분류율이 떨어지고, 트리가 복잡해지는 문제가 발생한다. 신경망 등의 알고리즘이 여러 변수를 동시에 고려하지만 결정트리는 한 개의 변수만을 선택하기 때문에 발생하는 당연한 문제이다.

    Overfitting에 빠지기 쉽다.

- **K- Nearest Neighbor (KNN)**

  지도학습의 일종으로,어떤 데이터가 주어지면 그 주변(이웃)의 데이터를 살펴본 뒤 더 많은 데이터가 포함되어 있는 범주로 분류하는 알고리즘

  - 장점

    샘플 수가 많을 때 좋은 분류법이다.

    비모수적 방법이기 때문에 어떤 분포든 상관 없음

    > 비모수적 방법: 자료가 정규분포가 아니거나 표본의 크기가 작으면 분포에 대한 기본가정을 필요로 하지 않는 통계적 기법이다.

  - 단점

    > 데이터가 많을 때 분석속도가 느릴 수 있음.
    >
    > 특정분포를 가정하지 않기 때문에 샘플수가 많이 있어야 정확도가 좋다.

- **K- means**

  비지도학습의 일종으로, unlabeled 데이터의 속성을 파악하여 비슷한 유형의 데이터를 그룹화 하는 알고리즘 
  
  - 장점
  
    주어진 데이터의 내부구조에 대한 사전적인 정보 없이 의미 있는 자료구조를 찾아낼 수 있는 방법이다.
  
  - 단점
  
    만일 군집수 k 가 원 데이터구조에 적합하지 않으면 좋은 결과를 얻을 수 없다.

---

##### 3. 딥러닝

딥러닝이란 심층 신경망을 사용해서 기계를 학습시키는 알고리즘을 뜻한다. 여러 머신러닝 기법 중 하나다.

- 머신러닝과의 차이점

  머신 러닝 알고리즘은 모델이 부정확한 예측을 반환하면 엔지니어가 개입하여 조정해야 한다.

  딥러닝 알고리즘은 자체 신경망을 통해 스스로 판단해서 학습한다.

---

##### 4. 데이터 마이닝

데이터 마이닝은 대규모로 저장된 데이터 안에서 체계적이고 자동적으로 통계적 규칙이나 패턴을 분석하여 가치있는 정보를 추출하는 과정

- 머신러닝과의 차이

  목적이 다르다. 

  데이터 마이닝은 가지고 있는 데이터에서 현상 및 특성을 발견하는 것이 목적이다.

  머신러닝은 기존 데이터를 통해 학습을 시킨 후 새로운 데이터에 대한 예측값을 알아내는 것이 목적이다.

---

##### 5. 빅 데이터

빅 데이터는 대량의 데이터셋(빅 데이터)을 수집하고 분석하는 기술.

- 머신러닝과의 차이

  목적이 다르다.

  빅 데이터는 조직에 도움이 되는 대용량 데이터에서 유용한 숨겨진 패턴을 발견하는 것이 목적이다.

  머신러닝은 기존 데이터를 통해 학습을 시킨 후 새로운 데이터에 대한 예측값을 알아내는 것이 목적이다.

---

##### 5. 학습 방법

- **Supervised Learning**(지도 학습 )

  정답에 해당하는 Label(또는 Class) 데이터를 통해 학습을 하는 방법이다. 

  분류, 회귀 모델에 사용된다.

- **Unsupervised Learning**(비지도 학습 )

  정답을 따로 알려주지 않는 학습 방법이다. 정답이 없기 때문에 Input Data를 기반으로 군집을 찾는 학습을 진행한다.

- **Reinforcement Learning**(강화 학습)

  주변 상태에 따라 어떤 행동을 할지 판단을 내리는 주체인 에이전트가 있고, 에이전트가 속한 환경이 있다. 에이전트가 행동을 하면 그에 따라 상태가 바뀌게 되고, 보상을 받을 수도 있다. 강화학습의 목표는 주어진 환경에서 보상을 최대한 많이 받을 수 있는 에이전트를 학습하는 것.

---

##### 6. 분류와 회기, 군집

- 분류

  Class를 예측하는 방법이다. 데이터가 어떤 Class에 속하는지 예측하는 것이다.

- 회귀

  확률을 예측하는 것이 아니다!

  회귀는 연속된 값을 예측하는 것이다. 연속성이 있는 출력이 있을 때, 다음에 어떤 출력을 할지 예측하는 것이다.

- 군집

  unlabeled 데이터 안에서 패턴과 구조를 발견해서 비슷한 속성을 가진 데이터들끼리 묶어주는 것.

---

### 구현

##### 1. 퍼셉트론(뉴런이라고도 함, Perceptron)

실제 뇌의 신경세포 뉴런의 동작을 모방한 알고리즘으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘이다. 

각각의 입력값에는 각각의 가중치가 존재하고, 가중치가 클수록 해당 입력 값이 중요하다는 것을 의미한다.

입력값과 가중치를 곱한 각각의 값의 전체 합이 퍼셉트론의 임계치를 넘으면 1을 출력한다. (그 외에는 0을 출력한다.)

> 임계치(threshold): 퍼셉트론이 활성화 되기 위한 최소값
>
> 가중치(weight): 선형 경계의 방향성 또는 형태를 나타내는 값
>
> 바이어스(bias): 참값과 추정값의 차이
>
> net 값: 입력값과 가중치를 곱한 각각의 값의 전체 합

---

##### 2. 단층 퍼셉트론(Single-layer Perceptron)

여러 퍼셉트론이 하나의 층을 이루고 있는 구조로 입력 벡터를 두 부류로 구분하는 선형분류기이다. 이를 통해 AND, NAND, OR게이트를 구현할 수 있다.

XOR게이트는 비선형 분류이기 때문에 구현을 할 수 없다.

---

##### 3. 다층 퍼셉트론(Multi-layer Perceptron)

입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 둔 구조이다.

AND, NAND, OR게이트를 조합하여 XOR게이트를 구현하는 것 처럼, 비선형 분류를 구현하기 위한 구조이다.

> 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN) 이라고 하며, 학습을 시키는 신경망이 심층 신경망일 경우 이를 **딥러닝(Deep Learning)**이라고 한다.
>
> Multi-layer Perceptron을 artificial neural network, ANN라고 한다.

- ##### **문제점 1. 비선형 분류 구현의 한계**

  Hidden layer도 무작정 쌓기만 한다고 해서 퍼셉트론을 선형분류기에서 비선형분류기로 바꿀 수 있는 것은 아니다.  선형 시스템이 아무리 깊어지더라도 f(ax+by)=af(x) + bf(y)의 성질에 의해 linear한 연산을 갖는 layer를 수십개 쌓아도 결국 이는 하나의 linear 연산으로 나타낼 수 있기 때문이다.

  **해결방법**: 활성화 함수를 사용한다.

  > 아래에서 설명

- **문제점 2. 학습이 불가능**

  Oupput Layer의 Target value가 있는 것과 달리 중간의 Hidden Layer가 가지고 있는 node에는 Target Value가 존재하지 않아 은닉층의 오차(목표값과 실측값의 차이)를 구하기 난해하여 학습을 시킬 방법이 없었다.

  **해결 방법**: 학습 방법으로 델타 규칙(Delta rule)을 사용한다.

  >  **델타 규칙**이란 오차를 줄이기 위해 가중치를 체계적으로 변경하는 방법을 의미한다. 
  >
  > 방법은, 어떤 입력 노드가 출력 노드의 오차에 기여했다면, 두 노드의 연결 가중치는 해당 입력 노드의 출력과 출력 노드의 오차에 비례해 조절한다.
  >
  > 이런 델타 규칙을 output에서 input방향으로 진행하며 Weight를 재업데이트 하는 것을 **역전파(Back Propagation)**라고 한다.

- **문제점 3. Overfitting**

  복잡한 문제를 해결하기 위해 Hidden Layer를 늘림으로 인해 학습이 트레이닝 셋에 너무 과최적화되어 실제 데이터를 집어 넣어 분류를 하려고 하면 정확도가 좀 떨어지는 것을 의미한다.

  **해결방법**: Regularization  (Overfitting을 방지하는 기법을 총칭한 용어)

  1. **Dropout(딥러닝에서 대표적으로 사용됨)**

  > Dropout은 학습 과정에서 일부 뉴런을 사용하지 않는 형태로 만들어서 오버피팅을 방지한다.

  2. **교차 검증(Cross validation)**

     훈련 데이터 셋 전체를 한 번에 훈련시키지 않고 일부를 남겨두고, 남겨둔 데이터는 테스트하는 것에 사용하는 방법이다.

     - 장점: 평가에 사용되는 데이터 편중을 막을 수 있다. 정확도를 향상시킬 수 있다.
     - 단점: Iteration(반복)횟수가 많기 때문에 모델 훈련/평가 시간이 오래 걸린다.

- **문제점 4. Vanishing Gradient Problem(기울기값이 사라지는 문제)**

  복잡한 문제를 해결하기 위해 다수의 Hidden Layer가 있을 때, 역전파가 진행돼는 동안 각각의 노드에 Delta rule이 적용되고 오차가 거의 0인 뉴런에 의해 입력층 쪽으로갈수록 대부분의 노드에서 기울기가 0이되어 결국 gradient가 거의 완전히 사라지는 현상. 결국 입력층쪽 노드들은 기울기가 사라지므로 학습이 되지 않게 된다. 

  **해결방법**: 활성화 함수를 변경한다.

  > Vanishing Gradient Problem은 활성화 함수가 Sigmoid일때 발생하기 때문에 활성화 함수를 ReLU로 바꿔준다. 대부분의 input값에 대해 기울기가 0이 아니기 때문에 Vanishing Gradient Problem가 발생하지 않는다.

---

##### 4. 활성화 함수(activation function)

최종출력 신호를 다음 뉴런으로 보내줄지 말지 결정하는 역할을 한다.

이를 통해 퍼셉트론의 입력값에 대한 출력값이 linear하게 나오지 않게 결정해주는 함수이다.

종류

- **Sigmoid**

  일정 값을 기준으로 0인지 1인지구분함으로써 분류하는 방식으로 binary classification(이항 분류)에 적절함 함수다.

  Vanishing Gradient Problem가 발생할 수 있다.

- **Tanh**

  sigmoid가 갖고 있던 최적화 과정에서 느려지는 문제를 해결했다.

  Vanishing Gradient Problem가 발생할 수 있다.

- **ReLU**

  0보다 작은 값이 나온 경우 0을 반환하고, 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수다.

- **leaky ReLU**

  0보다 작은 값이 나온 경우 그 값에 0.01을 곱한 값을 반환하고, 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수다.

  leaky ReLU를 일반적으로 많이 쓰진 않지만 ReLU보다 학습이 더 잘 되긴 한다.

- **Softmax**

  입력받은 값을 0~1사이의 값으로 모두 정규화하며 출력한다. 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다.

  분류하고자 하는 클래스가 여러개 일 때 확률을 표현하기 위해 사용한다.

---

##### 5. 모델 정의(Modelling)

모든 머신러닝 모델은 다음 3가지 과정을 거친다.

1. **가설 정의**: 학습하고자 하는 가설을 수학적 표현식으로 나타낸다.
2. **손실함수 정의**: 가설의 성능을 측정할 수 있는 손실함수를 정의한다.
3. **최적화 정의**: 손실함수를 최소화 할 수 있는 학습 알고리즘을 설계한다.

---

##### 6. 가설(Hypothesis)

학습하고자 하는 가설을 수학적 표현식으로 나타낸다.

> 예를 들어 가설을 y = Wx+b라고 할 때
>
> 이 때 x와 y는 데이터로부터 주어지는 인풋 데이터, 타겟 데이터
>
> W는 Weight, b는 bias를 의미하며 트레이닝 데이터로부터 학습을 통해 적절한 값을 찾아내야한 하는 값이다.  W와 b는 파라미터라고 부른다.

---

##### 7. 손실함수(cost function)

가설로 설정한 파라미터값을 학습하기 위해 현재 파라미터값이 목적에 적합한 값인지 측정하기 위해 사용한다.

손실함수는 우리의 목적에 가까운 형태로 파라미터가 최적화 되었을 때(모델이 잘 학습되었을 때) 더 작은 값을 갖는 특성을 가지고 있다. (그렇기 때문에 손실함수는 비용함수라고도 불린다.)

- **Mean Squared Error, MSE(평균제곱오차)**

  오차의 제곱을 합하고 평균을 취한 것이다.  

  추측값에 대한 정확성 측정 방법으로 사용된다.

- **Cross Entropy Error, CEE(크로스 엔트로피)**

  예측값에 자연로그를 취한 후 타겟 데이터 값과 곱한 값을 전부 합한 후 음수로 변환한다.

  오직 실제 정답과의 오차만을 파악한다.

  분류해야 할 클래스가 3개 이상인 경우 사용하며 타겟 데이터가 one-hot 형태로 제공되야 한다.

---

##### 8. 최적화(optimizer)

손실함수를 최소화하는 방향으로 파라미터들을 업데이트 할 수 있는 학습 알고리즘이다.

업데이트 방법 종류

- **Gradient Descent(경사하강법)**

  제시된 cost function의 1차 미분계수를 이용해 cost function의 최소값을 찾아가는 알고리즘이다.

  - Batch Gradient Descent, BGD

    트레이닝 데이터의 평균 미분값으로 파라미터의 한 스텝 업데이트를 진행하는 방법. (최적의 한 스텝을 나아간다. 시간이 오래걸린다.)

  - Stochastic Gradient Descent, SGD

    트레이닝 데이터를 특정 갯수 씩 묶은 후(Mini-Batch) BGD를 진행한다.

- **Momentum**

  SGD보다 빠른학습속도를 얻고, local minima를 문제를 개선하고자 SGD에 관성의 개념을 적용했다. 진행 방향에 가속도를 붙여주어 빠르게 local minima를 벗어나고, global minima에 도달하기 위한 기법이다. 

  > local minima: 최소값이 아닌, 단순 기울기가 0이 되는 지점에서 멈추는 것
  >
  > global minima: 함수의 진짜 최소값

  이전 이동거리에 관성계수를 곱한 값에 SGD값을 더해서 한 스텝을 나아간다. 

  Momentum + SGD   //  (가속도 = 0이면 SGD)

- **Nesterov Accelrated Gradient,NAG**

  최적의 parameter를 관성에 의해 지나칠 수 있는 Momentum의 문제점을 해결하기 위한 기법이다.  기존의 Momentum에서 기울기를 적용할 땐 업데이트 된 파라미터의 SGD를 사용했지만, NAG는 관성에 의해 이동된 곳에서의 파라미터의SGD 적용한다.

  Momentum + SGD(Momentum)

- **Adaptive Gradient (Adagrad)**

  동일 기준으로 update되던 각각의 parameter에 개별 기준을 적용하는 방법이다. 

  지속적으로 변화하던 parameter는 최적값에 가까워졌을것이고 한 번도 변하지 않은 parameter는 더 큰 변화를 줘야한다는 것이 Adagrad의 개념이다.

- **RMSProp**

  학습이 진행됨에 따라 변화 폭이 눈에 띄게 줄어들어 결국 움직이지 않게 되는 Adagrad의 문제점을 해결하기 위한 기법이다. 

  Adagrad의 계산식에 지수이동평균을 적용해서 학습에 필요한 최소 step은 유지할 수 있게 했다.

  > 지수이동평균: 최근 값을 더 잘 반영하기 위해 최근 값과 이전 값에 각각 가중치를 주어 계산한는 방법

- **Adaptive Moment Estimation (Adam)**

  RMSProp와 Momentum 기법을 합친 optimizer로, SGD에 관성의 개념을 적용하고 이후 지수이동평균을 적용하는 기법이다.

---

##### 9. Neural Network Model

학습 알고리즘이다.

**장단점**

- 장점: 대량의 데이터 세트를 처리할 수 있다.

- 단점: black-box nature

  신경망에 의해 어떤 특정한 결과가 도출 될 때마다 어떻게 혹은 왜 이것을 도출해 냈는지 알 수 없다.

**종류**

- **artificial neural network, ANN**

  다층 퍼셉트론

- **Convolution neural network, CNN**

  Convolution이라는 전처리 작업이 들어가는 Neural Network 모델로 딥러닝에서 주로 이미지나 영상 데이터를 처리할 때 쓰인다.

  일반 ANN은 기본적으로 1차원 형태의 데이터를 사용하기 때문에 (예를들면 1028x1028같은 2차원 형태의)이미지가 입력값이 되는 경우, 이것을 flatten시켜서 한줄 데이터로 만들어야 하는데 이 과정에서 이미지의 공간적/지역적 정보(spatial/topological information)가 손실된다. 또한 추상화과정 없이 바로 연산과정으로 넘어가 버리기 때문에 학습시간과 능률의 효율성이 저하된다. 이러한 문제점에서부터 고안한 해결책이 CNN이다. CNN은 이미지를 날것(raw input) 그대로 받음으로써 공간적/지역적 정보를 유지한 채 특성(feature)들의 계층을 빌드업한다.

  **장단점**

  - 장점: 데이터의 지역정보를 효율적으로 볼 수 있다.
  - 단점: 멀리 있는 필셀 간의 관계를 보려면 layer를 많이 쌓아야 한다. (이미지가 크면 신경망이 깊어야 한다.)

  **동작 과정**

  > Convolution -> Poolng -> Flatten -> ANN -> Softmax Regression

  - **Convolution**

    커널을 이용해 컨볼루션을 수행해 이미지의 특징을 추출해내는 역할을 한다.

    커널을 통해 추출한 이미지에 Zero Padding 을 적용해 **활성화 맵(feature map)**을 만든다.  feature map은 원본 이미지에서 명확히 들어자니 않았던 특징들을 보여준다.   

    > 커널은 필터라고 이해하면 된다.

    - Zero Padding

      커널을 통해 추출한 이미지는 기존 이미지보다 크기가 줄어들기 때문에 손실되는 부분이 발생하는데, 이를 방지하기 위해 추출한 이미지의 가장자리에 0으로 구성된 데이터 n겹을 감싸 이미지의 크기를 기존 이미지와 똑같이 맞춘다.

    - Stride

      필터를 얼마만큼 움직여 주는가에 대한 값이다. 기본값은 1이다.

    - Flatten

      다차원 배열을 1차원으로 바꾸는 기법

  - **Poolng**

    풀링으로 **이미지의 차원을 축소함으로써 필요한 연산량을 감소**시킬 수 있고, **이미지의 가장 강한 특징만을 추출하는 특징 선별** 할 수 있다.

    최대값 풀링, 평균값 풀링, 최소값 풀링이 있다.

    (풀링은 하나의 필터라고 이해하자.)

- **Recurrent Neural Network, RNN**

  자연어 처리 문제에 주로 사용되는 인공신경망 구조로 시계열 데이터를 다루기에 최적화된 인공신경망이다.

  RNN은 기본적인 ANN구조에서 이전 시간(t-1)의 은닉층의 출력값을 다음 시간(t)에 은닉층의 입력값으로 다시 집어넣는 경로가 추가된 형태이다.
  
  **장단점**
  
  - 장점: 이전 상태에 대한 정보를 일종의 메모리 형태로 저장할 수 있어 시계열 데이터를 다룰 때 강력하다.
  - 단점: Vanishing Gradient Problem이 발생할 수 있다. 시간축 1에서 받은 영향력은 크게 남아있지만, 새로운 시간축에서 들어온 input데이터의 영향력에 의해 영향력이 반복해서 덮어씌워며 점점 약해진다.

- **Long Short-Term Memory, LSTM**

  LSTM은 은닉층을 각각의 노드가 input gate, forget date, output gate로 구성된 메모리 블럭이라는 조금 복잡한 구조로 대체한다. 

  **장단점**

  - 장점: 오랜 시간이 지난 데이터의 영향력을 보존할 수 있다.
  - 단점: 입력 데이터가 매우 길면 효율적으로 학습하지 못한다.

- **Gate Recurren Unit, GRU**

---

10. 