# 머신러닝 개념

##### 1. 퍼셉트론(뉴런이라고도 함, Perceptron)

실제 뇌의 신경세포 뉴런의 동작을 모방한 알고리즘으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘이다. 

각각의 입력값에는 각각의 가중치가 존재하고, 가중치가 클수록 해당 입력 값이 중요하다는 것을 의미한다.

입력값과 가중치를 곱한 각각의 값의 전체 합이 퍼셉트론의 임계치를 넘으면 1을 출력한다. (그 외에는 0을 출력한다.)

> 임계치(threshold): 퍼셉트론이 활성화 되기 위한 최소값
>
> 가중치(weight): 선형 경계의 방향성 또는 형태를 나타내는 값
>
> 바이어스(bias): 참값과 추정값의 차이
>
> net 값: 입력값과 가중치를 곱한 각각의 값의 전체 합

---

##### 2. 단층 퍼셉트론(Single-layer Perceptron)

여러 퍼셉트론이 하나의 층을 이루고 있는 구조로 입력 벡터를 두 부류로 구분하는 선형분류기이다. 이를 통해 AND, NAND, OR게이트를 구현할 수 있다.

XOR게이트는 비선형 분류이기 때문에 구현을 할 수 없다.

---

##### 3. 다층 퍼셉트론(Multi-layer Perceptron)

입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 둔 구조이다.

AND, NAND, OR게이트를 조합하여 XOR게이트를 구현하는 것 처럼, 비선형 분류를 구현하기 위한 구조이다.

> 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN) 이라고 하며, 학습을 시키는 신경망이 심층 신경망일 경우 이를 **딥러닝(Deep Learning)**이라고 한다.
>
> Multi-layer Perceptron을 artificial neural network, ANN라고 한다.

- ##### **문제점 1. 비선형 분류 구현의 한계**

  Hidden layer도 무작정 쌓기만 한다고 해서 퍼셉트론을 선형분류기에서 비선형분류기로 바꿀 수 있는 것은 아니다.  선형 시스템이 아무리 깊어지더라도 f(ax+by)=af(x) + bf(y)의 성질에 의해 linear한 연산을 갖는 layer를 수십개 쌓아도 결국 이는 하나의 linear 연산으로 나타낼 수 있기 때문이다.

  **해결방법**: 활성화 함수를 사용한다.

  > 아래에서 설명

- **문제점 2. 학습이 불가능**

  Oupput Layer의 Target value가 있는 것과 달리 중간의 Hidden Layer가 가지고 있는 node에는 Target Value가 존재하지 않아 은닉층의 오차(목표값과 실측값의 차이)를 구하기 난해하여 학습을 시킬 방법이 없었다.

  **해결 방법**: 학습 방법으로 델타 규칙(Delta rule)을 사용한다.

  >  **델타 규칙**이란 오차를 줄이기 위해 가중치를 체계적으로 변경하는 방법을 의미한다. 
  >
  > 방법은, 어떤 입력 노드가 출력 노드의 오차에 기여했다면, 두 노드의 연결 가중치는 해당 입력 노드의 출력과 출력 노드의 오차에 비례해 조절한다.
  >
  > 이런 델타 규칙을 output에서 input방향으로 진행하며 Weight를 재업데이트 하는 것을 **역전파(Back Propagation)**라고 한다.

- **문제점 3. Overfitting**

  복잡한 문제를 해결하기 위해 Hidden Layer를 늘림으로 인해 학습이 트레이닝 셋에 너무 과최적화되어 실제 데이터를 집어 넣어 분류를 하려고 하면 정확도가 좀 떨어지는 것을 의미한다.

  **해결방법**: 여러가지가 있지만 Dropout이 딥러닝에서 대표적으로 사용됨

  > Dropout은 학습 과정에서 일부 뉴런을 사용하지 않는 형태로 만들어서 오버피팅을 방지할 수 있도록 만들어주는 Regularization 기법이다.

- **문제점 4. Vanishing Gradient Problem(기울기값이 사라지는 문제)**

  복잡한 문제를 해결하기 위해 다수의 Hidden Layer가 있을 때, 역전파가 진행돼는 동안 각각의 노드에 Delta rule이 적용되고 오차가 거의 0인 뉴런에 의해 입력층 쪽으로갈수록 대부분의 노드에서 기울기가 0이되어 결국 gradient가 거의 완전히 사라지는 현상. 결국 입력층쪽 노드들은 기울기가 사라지므로 학습이 되지 않게 된다. 

  **해결방법**: 활성화 함수를 변경한다.

  > Vanishing Gradient Problem은 활성화 함수가 Sigmoid일때 발생하기 때문에 활성화 함수를 ReLU로 바꿔준다. 대부분의 input값에 대해 기울기가 0이 아니기 때문에 Vanishing Gradient Problem가 발생하지 않는다.

---

##### 4. 활성화 함수(activation function)

최종출력 신호를 다음 뉴런으로 보내줄지 말지 결정하는 역할을 한다.

이를 통해 퍼셉트론의 입력값에 대한 출력값이 linear하게 나오지 않게 결정해주는 함수이다.

종류

- **Sigmoid**

  일정 값을 기준으로 0인지 1인지구분함으로써 분류하는 방식으로 binary classification(이항 분류)에 적절함 함수다.

  Vanishing Gradient Problem가 발생할 수 있다.

- **Tanh**

  sigmoid가 갖고 있던 최적화 과정에서 느려지는 문제를 해결했다.

  Vanishing Gradient Problem가 발생할 수 있다.

- **ReLU**

  0보다 작은 값이 나온 경우 0을 반환하고, 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수다.

- **leaky ReLU**

  0보다 작은 값이 나온 경우 그 값에 0.01을 곱한 값을 반환하고, 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수다.

  leaky ReLU를 일반적으로 많이 쓰진 않지만 ReLU보다 학습이 더 잘 되긴 한다.

- **Softmax**

  입력받은 값을 0~1사이의 값으로 모두 정규화하며 출력한다. 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다.

  분류하고자 하는 클래스가 여러개 일 때 확률을 표현하기 위해 사용한다.

---

##### 5. 모델 정의(Modelling)

모든 머신러닝 모델은 다음 3가지 과정을 거친다.

1. **가설 정의**: 학습하고자 하는 가설을 수학적 표현식으로 나타낸다.
2. **손실함수 정의**: 가설의 성능을 측정할 수 있는 손실함수를 정의한다.
3. **최적화 정의**: 손실함수를 최소화 할 수 있는 학습 알고리즘을 설계한다.

---

##### 6. 가설(Hypothesis)

학습하고자 하는 가설을 수학적 표현식으로 나타낸다.

> 예를 들어 가설을 y = Wx+b라고 할 때
>
> 이 때 x와 y는 데이터로부터 주어지는 인풋 데이터, 타겟 데이터
>
> W는 Weight, b는 bias를 의미하며 트레이닝 데이터로부터 학습을 통해 적절한 값을 찾아내야한 하는 값이다.  W와 b는 파라미터라고 부른다.

---

##### 7. 손실함수(cost function)

가설로 설정한 파라미터값을 학습하기 위해 현재 파라미터값이 목적에 적합한 값인지 측정하기 위해 사용한다.

손실함수는 우리의 목적에 가까운 형태로 파라미터가 최적화 되었을 때(모델이 잘 학습되었을 때) 더 작은 값을 갖는 특성을 가지고 있다. (그렇기 때문에 손실함수는 비용함수라고도 불린다.)

- **Mean Squared Error, MSE(평균제곱오차)**

  오차의 제곱을 합하고 평균을 취한 것이다.  

  실제 정답에 대한 정답률의 오차뿐만 아니라 다른 오답들에 대한 정답률 오차또한 포함하여 계산한다.

  추측값에 대한 정확성 측정 방법으로 사용된다.

- **Cross Entropy Error, CEE(크로스 엔트로피)**

  예측값에 자연로그를 취한 후 타겟 데이터 값과 곱한 값을 전부 합한 후 음수로 변환한다.

  오직 실제 정답과의 오차만을 파악한다.

  분류해야 할 클래스가 3개 이상인 경우 사용하며 타겟 데이터가 one-hot 형태로 제공되야 한다.

---

##### 8. 최적화(optimizer)

손실함수를 최소화하는 방향으로 파라미터들을 업데이트 할 수 있는 학습 알고리즘이다.

업데이트 방법 종류

- **Gradient Descent(경사하강법)**

  제시된 cost function의 1차 미분계수를 이용해 cost function의 최소값을 찾아가는 알고리즘이다.

  - Batch Gradient Descent, BGD

    트레이닝 데이터의 평균 미분값으로 파라미터의 한 스텝 업데이트를 진행하는 방법. (최적의 한 스텝을 나아간다. 시간이 오래걸린다.)

  - Stochastic Gradient Descent, SGD

    트레이닝 데이터를 특정 갯수 씩 묶은 후(Mini-Batch) BGD를 진행한다.

- **Momentum**

  SGD보다 빠른학습속도를 얻고, local minima를 문제를 개선하고자 SGD에 관성의 개념을 적용했다. 진행 방향에 가속도를 붙여주어 빠르게 local minima를 벗어나고, global minima에 도달하기 위한 기법이다. 

  > local minima: 최소값이 아닌, 단순 기울기가 0이 되는 지점에서 멈추는 것
  >
  > global minima: 함수의 진짜 최소값

  이전 이동거리에 관성계수를 곱한 값에 SGD값을 더해서 한 스텝을 나아간다. 

  Momentum + SGD   //  (가속도 = 0이면 SGD)

- **Nesterov Accelrated Gradient,NAG**

  최적의 parameter를 관성에 의해 지나칠 수 있는 Momentum의 문제점을 해결하기 위한 기법이다.  기존의 Momentum에서 기울기를 적용할 땐 업데이트 된 파라미터의 SGD를 사용했지만, NAG는 관성에 의해 이동된 곳에서의 파라미터의SGD 적용한다.

  Momentum + SGD(Momentum)

- **Adaptive Gradient (Adagrad)**

  동일 기준으로 update되던 각각의 parameter에 개별 기준을 적용하는 방법이다. 

  지속적으로 변화하던 parameter는 최적값에 가까워졌을것이고 한 번도 변하지 않은 parameter는 더 큰 변화를 줘야한다는 것이 Adagrad의 개념이다.

- **RMSProp**

  학습이 진행됨에 따라 변화 폭이 눈에 띄게 줄어들어 결국 움직이지 않게 되는 Adagrad의 문제점을 해결하기 위한 기법이다. 

  Adagrad의 계산식에 지수이동평균을 적용해서 학습에 필요한 최소 step은 유지할 수 있게 했다.

  > 지수이동평균: 최근 값을 더 잘 반영하기 위해 최근 값과 이전 값에 각각 가중치를 주어 계산한는 방법

- **Adaptive Moment Estimation (Adam)**

  RMSProp와 Momentum 기법을 합친 optimizer로, SGD에 관성의 개념을 적용하고 이후 지수이동평균을 적용하는 기법이다.

---

##### 9. 학습 방법

- **Supervised Learning**(지도 학습 )

  정답에 해당하는 Label(또는 Class) 데이터를 통해 학습을 하는 방법

  분류 모델에 사용된다.

- **Unsupervised Learning**(비지도 학습 )

  정답을 따로 알려주지 않는 학습 방법이다.

  Input Data를 기반으로 군집을 찾는 학습을 진행

- **Reinforcement Learning**(강화 학습)

  정의된 주체(agent)가 현재의 상태(state)를 관찰하여 선택할 수 있는 행동(action)들 중에서 가장 최대의 보상(reward)을 가져다주는지 행동이 무엇인지를 학습하는 방법

  강화 학습은 주체(agent)가 환경으로부터 보상을 받음으로써 학습하기 때문에 지도 학습과 유사해 보이지만, 사람으로부터 학습을 받는 것이 아니라 변화되는 환경으로부터 보상을 받아 학습한다는 점에서 차이가 있다.

---

##### 10. Neural Network Model

학습 알고리즘이다.

- **artificial neural network, ANN**

  다층 퍼셉트론

- **Convolution neural network, CNN**

  Convolution이라는 전처리 작업이 들어가는 Neural Network 모델로 딥러닝에서 주로 이미지나 영상 데이터를 처리할 때 쓰인다.

  일반 ANN은 기본적으로 1차원 형태의 데이터를 사용하기 때문에 (예를들면 1028x1028같은 2차원 형태의)이미지가 입력값이 되는 경우, 이것을 flatten시켜서 한줄 데이터로 만들어야 하는데 이 과정에서 이미지의 공간적/지역적 정보(spatial/topological information)가 손실된다. 또한 추상화과정 없이 바로 연산과정으로 넘어가 버리기 때문에 학습시간과 능률의 효율성이 저하된다. 이러한 문제점에서부터 고안한 해결책이 CNN이다. CNN은 이미지를 날것(raw input) 그대로 받음으로써 공간적/지역적 정보를 유지한 채 특성(feature)들의 계층을 빌드업한다.

  동작 과정

  > Convolution -> Poolng -> Flatten -> ANN -> Softmax Regression

  - **Convolution**

    커널을 이용해 컨볼루션을 수행해 이미지의 특징을 추출해내는 역할을 한다.

    커널을 통해 추출한 이미지에 Zero Padding 을 적용해 **활성화 맵(feature map)**을 만든다.  feature map은 원본 이미지에서 명확히 들어자니 않았던 특징들을 보여준다.   

    > 커널은 필터라고 이해하면 된다.

    - Zero Padding

      커널을 통해 추출한 이미지는 기존 이미지보다 크기가 줄어들기 때문에 손실되는 부분이 발생하는데, 이를 방지하기 위해 추출한 이미지의 가장자리에 0으로 구성된 데이터 n겹을 감싸 이미지의 크기를 기존 이미지와 똑같이 맞춘다.

    - Stride

      필터를 얼마만큼 움직여 주는가에 대한 값이다. 기본값은 1이다.

    - Flatten

      다차원 배열을 1차원으로 바꾸는 기법

  - **Poolng**

    풀링으로 **이미지의 차원을 축소함으로써 필요한 연산량을 감소**시킬 수 있고, **이미지의 가장 강한 특징만을 추출하는 특징 선별** 할 수 있다.

    최대값 풀링, 평균값 풀링, 최소값 풀링이 있다.

    (풀링은 하나의 필터라고 이해하자.)

- **Recurrent Neural Network, RNN**

  자연어 처리 문제에 주로 사용되는 인공신경망 구조로 시계열 데이터를 다루기에 최적화된 인공신경망이다.

  RNN은 기본적인 ANN구조에서 이전 시간(t-1)의 은닉층의 출력값을 다음 시간(t)에 은닉층의 입력값으로 다시 집어넣는 경로가 추가된 형태이다.