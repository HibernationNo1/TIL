# 가설, 손실함수, 최적화

모든 머신러닝 모델은 다음 3가지 과정을 거친다.

1. **가설 정의**: 학습하고자 하는 가설을 수학적 표현식으로 나타낸다.
2. **손실함수 정의**: 가설의 성능을 측정할 수 있는 손실함수를 정의한다.
3. **최적화 정의**: 손실함수를 최소화 할 수 있는 학습 알고리즘을 설계한다.



선형 회귀(선형 함수를 이용해서 회귀를 수행하는 기법) 모델에서의 3가지 과정 예시

1. **가설 정의**: 학습하고자 하는 가설을 y = Wx+b 형태로 표현

   > 이 때 x와 y는 데이터로부터 주어지는 인풋 데이터, 타겟 데이터
   >
   > W와 b는 파라미터라고 부르며 트레이닝 데이터로부터 학습을 통해 적절한 값을 찾아내야한 하는 값

2. **손실함수(cost function) 정의**: 적절한 파라미터값을 알아내기 위해서는 현재 파라미터값이 우리가 풀고자 하는 목적에 적합한 값인지 측정이 가능해야 한다. 이를 위해 손실함수를 정의해야 한다.

   > 손실함수는 여러가지 형태로 정의될 수 있다. 그 중 가장 대표적인 손실함수 중 하나는 **평균제곱오차**(MSE)이다.
   >
   > 평균제곱오차는 다음 수식으로 정의된다.
   >
   >
   > $$
   > MSE = \frac {1} {2n} \sum_{j=1}^{n}(\widehat{y_i} - y_i)^{2}
   > $$
   > $$
   > \widehat{y_i} : 모델의\ 예측\ 값, \ \  	y_i : 정답\ 값, \ \ \frac {1} {2n}: 평균 \ 을\ 의미함
   > $$
   >
   > **예시** 
   >
   > 1. (y = [1, 10, 13, 7], 	 \widehat{y} = [10, 3, 1, 4] 일 때)
   >
   > $$
   > MSE = \frac {1}{2*4}\left \{ (10-1)^2+(3-10)^2+(1-13)^2+(4-7)^2 \right\} = 35.375
   > $$
   >
   > 
   >
   > 2. (y = [1, 10, 13, 7], 	 \widehat{y} = [2, 10, 11, 6] 일 때)
   >
   > $$
   > MSE = 1.5
   > $$
   >
   > 손실함수는 우리가 풀고자 하는 목적에 가까운 형태로 파라미터가 최적화 되었을 때(모델이 잘 학습되었을 때) 더 작은 값을 갖는 특성을 가지고 있다. (그렇기 때문에 손실함수는 비용함수라고도 불린다.)

3. **최적화(optimizer) 정의**: 손실함수를 최소화하는 방향하으로 파라미터들을 업데이트 할 수 있는 학습 알고리즘을 설계한다. 

   > 머신러닝 모델은 보통 맨 처음에 랜덤한 값으로 파라미터를 초기화한 후에 파라미터를 적절한 값으로 계속해서 업데이트한다. (사람이 지정해줘야 하는 파라미터는 하이퍼 파라미터로 불린다.)
   >
   > 이때 파라미터를 적절한 값으로 업데이트하는 알고리즘을 최적화 기업이라고 한다. 여러 최적화 기법 중에서 대표적인 기법은 **경사하강법(Gradient Descent)**이다.
   >
   > - 경사하강법: 현재 스텝의 파라미터에서 손실 함수의 미분값에 **러닝레이트** alpha를 곱한 만큼을 빼서 다음 스템의 파라미터값으로 지정하는 것. (러닝레이트는 사람이 지정해줘야하는 어떤 값)
   >
   >   > 손실함수의 미분값이 크면 하나의 스템에서 파라미터가 많이 업데이트되고 손실 함수의 미분값이 작으면 적게 업데이트 될 것이다. 
   >   >
   >   > 러닝레이트는 크게 설정하면 업데이트는 적게 되겠지만 정확도가 떨어지고 발산할(수렴하지 않는) 가능성이 높다.
   >   >
   >   > 러닝레이트를 작게 설정하면 정확도가 높아지겠지만 최적의 지점까지의 업데이트는 많이 이루어져야 한다. 또한 발산할 가능성은 적다.
   >
   >   경사하강법의 파라미터 한 스텝 업데이트 과정을 수식으로 나타내면 다음과 같다.
   >   $$
   >   \theta_{i+1} = \theta_i - \alpha \frac{\partial}{\partial \theta_i}Cost(\theta_0, \theta_1)
   >   $$
   >
   >   $$
   >   \theta : 우리가 \ 학습하고자\ 하는\ 파라미터 \\ \alpha  : 러닝레이트 \\ Cost(\theta_0, \theta_1): 손실함수
   >   $$
   >
   >   경사하강법은 손실 함수가 최소가 되는 지점에서 종료하는 것이 가장 이상적이민 현실적으로 언제 손실함수가 최소가 될지 알기 어렵기 때문에 충분한 횟수라고 생각되는 횟수만큼 업데이트를 진행한 수 학습을 종료한다.

