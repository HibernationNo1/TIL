# 모델의 성능 평가 지표 

## **분류 모델**

![](https://t1.daumcdn.net/cfile/tistory/99DC064C5BE056CE10)

- True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)

- False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)

- False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)

- True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)

  

### 1.**Accuracy(정확도)**

직관적으로 모델의 성능을 나타낼 수 있는 평가 지표.

전체 예측 건수에서 정답을 맞힌 건수의 비율
$$
Accuracy(정확도) = \frac{TP+TN}{TP+TN+FP+FN}
$$
단점: 정확도 역설(Accuracy Paradox)  

실제 데이터에 Negative 비율이 너무 높아서 희박한 가능성으로 발생할 상황에 대해서는 제대로 된 분류가 되지 않는다. ex) 눈이 1미터 이상 쌓이는 경우

### 2. **Precision(정밀도)**

모델이 True라고 분류한 것 중에서 실제 True인 것의 비율
$$
precision = \frac{TP}{TP+FP}
$$

단점: False이라고 판정한 경우 안에서 Ture인 경우(FN)를 놓치더라도 정확도가 100% 가 나올 수 있다.

### 3. **Recall(재현율)**

실제 True인 것 중에서 모델이 True라고 예측한 것의 비율
$$
Recall = \frac{TP}{TP+FN}
$$



단점: 언제나 Ture만 답하는 분류기에서는 Recall이 1이 된다. 

- precision - recall plot을 그릴 때 recall을 90%이상 유지 하면서 precision을 낮추지 않는게 중요하다.

  

### 4. **F1 score**

Precision과 Recall의 조화평균이다. 흔히 말하는 평균을 쓰지 않고 조화평균을 구하는 이유는 recall, precision 둘 중 하나가 0에 가깝게 낮을 때 지표에 그것이 잘 반영되도록, 다시말해 두 지표를 모두 균형있게 반영하여 모델의 성능이 좋지 않다는 것을 잘 확인하기 위함이다.

ex) recall, precision이 각각 1과 0.01이라는 값을 가지고 있다고 하자. 산술평균을 구하면 (1 + 0.01) / 2 = 0.505, 즉 절반은 맞히는 것처럼 보인다. 그러나 조화평균을 구해보면 2 * (1 * 0.01) / (1 + 0.01) = 0.019가 된다. 낮게 잘 나온다.
$$
F1\ score = 2* \frac{1}{\frac{1}{Precision}+ \frac{1}{Recall}} =2*\frac{Precision * Recall}{Precision+Recall}
$$



## mAP

CNN의 성능평가지표

object detection rate와 accuracy를 동시에 고려하는 성능평가지표

#### Precision-recall curve

- Precision : detection된 object에 대한 rate
- recall : label object중에서 detection된 object에 대한 rate

seccess detection를 결정하는 threshold값에 따라 precision과 recall값들도 달라진다. 

> threshold값이 높아지면 recall은 낮아지지만 precision은 높아진다.
>
> 반면 threshold값이 낮아지면 recall은 높아지지만 precision은 낮아진다.

이를 Precision-recall축으로 표현하면 curve로 표현될 수 있는데 이를 Precision-recall curve라고 하며, 특정 값의 점이 원점에서 멀 수록 성능이 좋다고 할 수 있다.

하지만 이는 어떤 algorithm의 performance를 전반적으로 파악하기에는 좋으나 서로 다른 두 algorithm의 성능을 quantitatively하게 비교하기에는 명확히 보이지 않는다. 그래서 나온 개념이 Average Precision이다.

#### Average Precision

Average Precision은 precision-recall curve에서 선 아래 면적으로 계산된다.

![](https://blog.kakaocdn.net/dn/bg73Gf/btquhjDtzFh/ETRORigF8P4dT02zB4kox1/img.png)

Average Precision이 높으면 높을수록 그 algorithm의 성능이 전체적으로 우수하다는 의미이다.

> 보통 계산 전에 PR-curve를 단조적으로 감소하는 그래프로 변경해준다. 이후 그래프 선 아래의 넓이를 계산함으로 AP를 구한다.

#### mean Average Precision

object class가 여러 개인 경우 각 class당 AP를 구한 후 평균을 낸다 이것이 바로 mAP이다.

> mAP는 MS COCO, VOC 등 dataset의 종류에 따라 그 수치가 다르게 나온다.

##### box mAP

Bbox로 표현되는 object detection에 대한 mAP이다.

##### mask mAP

segmentation으로 인해 표현되는 object detection에 대한 mAP이다.



## **회귀 모델**

### 1. **MAE(Mean Absolute Error)**

모델의 예측값과 실제값의 차이를 모두 더한다는 개념
$$
MAE = \frac{\sum|y-\widehat{y}|}{n}
$$
단점: 절대값을 취하기 때문에 모델이 실제보다 낮은 값 인지 높은 값 인지 알 수 없다.

### 2. **MSE (Mean Squared Error)**

제곱을 하기 때문에 MAE와는 다르게 모델의 예측값과 실제값 차이의 면적의 합이다.
$$
MSE = \frac{\sum(y-\widehat{y})^2}{n}
$$
단점: 제곱하기 때문에, 1미만의 에러는 더 작아지고, 그 이상의 에러는 더 커지는 값의 왜곡이 있다.

### 3. **RMSE (Root Mean Squared Error)**

MSE 에 Root 를 씌운 에러 지표.
$$
RMSE = \sqrt  \frac{\sum|y-\widehat{y}|}{n}
$$
장점: 제곱된 에러를 다시 루트로 풀어주기 때문에 에러를 제곱해서 생기는 값의 왜곡이 좀 덜하다.

### 4. **MAPE(Mean Absolute Percentage Error)**

MAE를 퍼센트로 변환한 것
$$
MAPE = \frac{\sum|y-\widehat{y}|}{n}*100 \%
$$
장점: 0%~100% 사이의 값을 가져 이해하기 쉬우므로 성능 비교 해석이 가능하다.