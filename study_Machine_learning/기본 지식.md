# Deep-learning

- **Ensemble이란 무엇인가?**

  동일한 학습 알고리즘을 사용해서 여러 model을 학습하는 개념이다.

  - **Bagging(Bootstrap Aggregation)**

    semple을 여러 번 뽑아(Bootstrap) 각 model을 독립적으로 학습시켜 결과를 집계(aggregating)하는 방법

  - **Boosting**

    semple을 여러 번 뽑아 1번 model이 학습하여 data에 weight가 부여되고, 해당 data를 2번 model이 학습하는 방법이 반복되어 모델 간 팀워크가 이루어지는 방법이다.

    잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복한다.



- **Augmentation이란?**

  image data를 좌-우 대칭 또는 crop등 여러 기법을 통해 변화를 주어 하나의 data로 여러 data로 증가시키는 기법

  





- **Precision-Recall curve란?**

  confidence lavel에 대한 threshold값에 따라 precision과 recall값들도 달라지는데, 이 때 precision값들과 recall값들의 관계를 그래프로 나타낸 것이 바로 precision-recall curve이다.

  object detecion의 performance evaluation방법 중 하나이다.

  장점: algorithm의 performance를 전반적으로 파악하기에는 좋다.

  단점: 서로 다른 두 algorithem의 성능을 quantitatively하게 비교하기에는 기준이 모호할 수 있다.

  

- **Average Precision이란?**

  algorithem의 성능을 하나의 값으로 표현한 것으로, Precision-Recall curve의 안쪽 면적으로 계산한다. Average Precision이 높으면 높을수록 그 algorithem의 성능이 우수하다는 의미이다.



- **mAP(mean Average Precision)란?**

  각 class당 AP의 합산을 class의 개수로 나눈 값이다.



- **Domain Adaptation(DA) 이란?**

  target domain에 label이 아예 없고 input image만 있다고 가정하면, 이전 task에서 배운 지식을 사용해 새로운 상황에서도 맞출 확률을 올려주는 기법이다.

  computer vision deep learning to classification분야에서는  challenging이라 한다.



- **challenging 이란? **

  image data의 수가 적음에도 불구하고 성공적으로 classificataion을 수행하는 방법. one shot, few shot learning과 같은 기법이 있다.

  augmentation과의 차이점 : augmentation은 같은 class에 대한 dataset을 늘리는 것이라 새로운 dataset이 database에 추가된다면 새롭게 학습해야 한다.



- **zero shot learning 이란?**

  한 번도 본 적 없는 data를 classification이 가능하도록 학습하는 것을 의미한다.

  이러한 학습 방법은 data가 없어도 유용한 패턴이나 결과를 도출하기 때문에 label이 없는 상품들을 분류해야 할 때나, 자동차 또는 전자 기기처럼 자주 신상품이 출시되는 품목에서 image들을 classification해야 할 때 사용된다.

  인간은 굳이 data를 검색하지 않아도 하나의 category의 물건을 몇 번 보면 같은 category의 새로운 class에 대해서 어떤 category인지 추론이 가능하다. 반면 AI는 해당 category에 대한 광범위한 information data를 필요로 한다. zero shot learning은 이러한 인간의 능력을 modeling하고자 하는 기술이다.



- **one shot learning 이란?**

  training example의 수가 적을 때 이를 인식하고 classification을 진행하는 방법이다.  A를 한 번만 보여주면, A와 다른 것들과의 구분을 하는 것이다. (주로 사람의 얼굴 구분에 사용됨)

  > N-way one-shot learning
  >
  > class가 N개인 one-shot learning을 의미한다.



- **few shot learning 이란?**

  episodic training을 통해 meta learning을 진행한다.

  **episodic training** : dataset을 query set이 support set으로 나눈 후 N-way K-Shot classification을 진행한다. 

  - **N-way K-Shot classification problem**

    ![](http://t1.kakaocdn.net/braincloud/homepage/article_image/4d1fb283-ba6d-4a76-89a3-52abc4994aaf.png)

    query set이 support set의 어떤 class에 해당되는지 학습하는 방법이다.

    > N-way K-Shot :
    >
    > N은 범주의 수, K는 범주별 서포트 데이터의 수를 의미한다. 



- **meta learning(learning to learn)이란?**

  몇몇 training 예제를 통해서 model로 하여금 새로운 기술을 배우거나, 새로운 환경에 빠르게 적응할 수 있도록 설계하는 것을 나타낸 것이다. 

  훌륭한 meta-learning model이라고 하면, training time동안에 접하지 않았던 새로운 task나 environment에 대해서 잘 적응하거나, 일반화가 잘 되는 것을 말한다.

  > 예시
  >
  > cat이 없는 dataset을 learning한 model도 cat이 포함 된 몇 개의 dataset을 본 후에는 test image상에 cat이 있는지 여부를 판단할 수 있다.

  접근 방식

  - metric기반의 efficient distance metric을 학습하는 방식
  - model기반의 external/internal memory를 통한 recurrent network을 사용하는 방식
  - optimization기반의 fast learning을 위한 model parameter를 최적화하는 방식





- **transfer learning이란?**

  pre-trained model을 기반으로 last output layer를 바꿔 학습하는 방법이다.

  trained model의 last output layer를 보유 중인 dataset에 대응하는 output layer로 바꾸고, 교체한 output layer의 앞의 소수 layer간의 결합 parameter를 소량의 data로 다시 학습 함으로써 input layer에 가까운 부분의 결합 parameters를 변화시키지 않는 것이다.



- **fine-tuning 이란?**

  기존에 학습되어져 있는 model을 기반을 architecture를 새로운 목적에 맞게 변형하고 weight of pre-trained model 로부터 학습을 업데이트 하는 방법을 말한다.

  이는 model의 parameters를 미세하게 조정하는 동작으로 진행된다.
  
  > pre-trained model의 일부 layer는 학습 시에 업데이트 되지 않게 하고, 일부 layer는 업데이트 되도록 설정한다.
  >
  > 다른 종류의 image라도 낮은 수주느이 특징은 상대적으로 비슷할 가능성이 높다. 때문에 fine-tuning과정에서 마지막 layer인 classifier과 가까운 layer부터 원하는 만큼만 학습 과정을 통해 업데이트한다.
  >
  > 이 때 freeze하는 layer의 수는 dataset의 크기와 pre-trained model에 사용된 dataset과의 유사성을 고려하여 결정한다.
  >
  > ![](https://media.vlpt.us/images/garam/post/575283e9-ef16-4f5a-a20d-31a12d00b271/finetuning_4.png)



- **instance segmentation 이란?**

  instance각각에 class안에서도 분류를 진행하는 segmentation기법

  localization을 수행한 후 그 box가 focus하고 있는 instance의 pixel이 궁금한 것이므로 각 box에 대해 image segmentation을 하는데 이때 동일 class여도 서로다른 instance이면 value를 갖지 않는다.

  > 보통 background에 대한 segmentation을 진행하지 않는다.
  
  ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FEznOQ%2FbtqCKzxqTKk%2FOxAw4ia27pjwxQ5BLryk6k%2Fimg.png)



- **Semantic Segmentation 이란?**

  instance의 class만 분류를 진행한다.

  class label이 10개 존재하는 경우 Semantic segmentation에서는 각 pixel들이 어떤 class에 포함되는지 안되는지를 10개의 class에 대해서 각각 binary하게 계산한다.

  > background까지 class로 포함하여 segmentation을 진행할 수 있다는 장점이 있다.

  ![](https://miro.medium.com/max/686/1*pa-PDx8PxNzeFtOecx8t_Q.png)
  
  > instance segmentation : 아래 그림의 왼쪽
  >
  > Semantic Segmentation : 아래 그림의 오른쪽
  
  ![](https://miro.medium.com/max/700/1*jHv5C23SLtL3UFeR8VnohQ.png)





- **panoptic segmentation 이란?**

  instance segmentation과 Semantic Segmentation의 장점을 수용해 만들어진 기법으로, background까지 포함하여 interest things까지 구분해서 instance segmentation을 수행한다.

  ![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSht3Zq-BBp6HwBA-t4LwVW_xiCSYRqEVK9X34WRW5ovDi8IYxgDhD7eRnkWgOeGjGeQac&usqp=CAU)

  





- **landmark localization(keypoint estimation) 이란?**

  instance에 대해서 중요하다고 생각되는 feature point를 landmark로 정의하고, 그걸들을 추정하고 추적하는 기법이다. 

  > landmark는 미리 정의를 해 놓은 특징들이다.
  >
  > 주로 사람의 얼굴이나 포즈를 추정하고 tracking하는데 많이 사용된다.

  ![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ83kr4hcCT4jZtpPw3qlN7D_YRjAB_w1yzMQ&usqp=CAU)









### network

#### ResNet(Residual Network)

network가 수십개의 layer가 되도록 깊어질수록 성능이 안좋아지는 것을 수백개의 layer까지 성능이 살아있을 수 있도록 한 Network



- **Residual Network가 왜 잘 되는가?**

  - Ensemble과 관련이 있는가?

    semple을 여러 번 뽑는 Bootstraping과정이 없지만 skip connection기법이 Bagging기법과 유사하기 때문에 관련이 있다고 생각한다.

    

#### GAN(Generative Adversarial Network)

결과물을 만드는 Generator와, 결과물을 평가하는 Generator를 평가하는 Discriminator가 서로 대립해서 성능을 점차 개선해 나가는 방향으로 학습하는 network

- Generator(생성자) 

  생성된 결과를 받아 실제 data와 비슷한 distribution을 가진 data를 만들어내도록 학습한다.

- Discriminator(구분자) 
  Generator가 만들어낸 결과물이 실제 data인지 가짜 data인지 구별해서 각각에 대한 확률을 추정한다.

  실제 데이터와 생성자가 생성한 가짜 데이터를 구별하도록 학습한다.





### Model

#### R-CNN

- **R-CNN**

  CPU상에서 Input image에 대해 Selective Search를 통해 ROI영역을 얻은 후 Support Vector Machine를 사용해 classification을 진행한다.

- **Fast R-CNN**

  CPU상에서 Input image에 대해 Selective Search진행하며 동시에 input image에 CNN을 적용하여 feature map에 ROI pooling을 적용한다.

  ROI pooling의 output인 feature map을 fully connected layer input으로 넣어 Bbox regression과 class classification을 진행한다.

  ​            ┌           CNN        	 	┐

  image │					   		     ├  RoI pooling ─ Fully connected

  ​            └  Selective Search 	┘

  ​                          1단                                 2단                    3단

- **Faster R-CNN**

  Image를 CNN에 통과시겨 Feature map을 얻는다.

  Feature map을 Region Proposal Network(RPN)의 input으로 사용하여 Region Proposals 과정을 수행한다.

  RPN의 output에 대해서 RoI pooling을 수행한다.

  ROI pooling의 output인 feature map을 fully connected layer input으로 넣는다. (이후 동일)

  

  image ─ CNN 		       	┐

  ​                   │		  	    	├  RoI pooling ─ Fully connected

  ​                   └       RPN   	┘

  ​                 1단      1.5단                2단                    3단

  - RPN

    feature map을 input으로 받아 object가 존재할법한 위치를 제안하도록 도와주는 Network

    ​																 ┌ Classification Layer

    image ─ CNN ─  Intermediate Layer ┤

    ​																 └ Regression Layer

    1. k개의 anchor box를 이용하며, 각각의 box로 image상에서 Intermediate Layer를 통해 sliding window를 수행해 Intermediate feature를 추출한다.
    2. Intermediate feature에 대해 Regression과 Classification을 수행한다.

- **Mask R-CNN**

  ​																						   ┌ Classification

  ​										   ┌  RPN  ┐				   ┌  FC  ┤

  image ─ ResNet ─ FPN  ┤  		  ├  ROI Align ┤ 		└ Regression

  ​										   └    ─    ┘					└  Mask Branch

  - FPN

    backbone network의 각각의 layer를 통해 얻어진 feature map을 사용해서 각각의 object의 feature를 최대한 보존한 feature map을 추출한다. 

  - Mask Branch

    각 영역의 class에 대한 이진 mask를 출력한다.





- **Recurrent Neural Network, RNN**

  Sequence data와 같이 시계열 데이터를 다루기에 최적화된 인공신경망이다.

  RNN은 기본적인 ANN구조에서 이전 시간(t-1)의 은닉층의 출력값을 다음 시간(t)에 은닉층의 입력값으로 다시 집어넣는 경로가 추가된 형태이다.

  **장점**: 이전 상태에 대한 정보를 일종의 메모리 형태로 저장할 수 있어 시계열 데이터를 다룰 때 강력하다.

  **단점**: Vanishing Gradient Problem이 발생할 수 있다. 시간 축 1에서 받은 영향력은 크게 남아있지만, 새로운 시간축에서 들어온 input데이터의 영향력에 의해 영향력이 반복해서 덮어씌워며 점점 약해진다.









