# 머신러닝 데이터

## 1. 개념

머신러닝에서 전체 데이터는 트레이닝 데이터, 검증용 데이터, 테스트 데이터로 나눈다.

머신러닝 모델은 크게 트레이닝 과정과 테스트 과정으로 나뉜다.  트레이닝과 테스트를 수행하기 위해서 가지고 있는 데이터 중 일부는 트레이닝 데이터, 일부는 테스트 데이터로 나눈다.

검증용 데이터는 트레이닝 과정에서 학습에 사용하지는 않지만 중간중간 테스트하는데 사용해서 학습하고 있는 모델이 오버피팅에 빠지지 않았는지 체크하는데 사용된다.

**검증용 데이터가 필요한 이유**

> 처음에는 트레이닝 에러와 검증 에러가 모두 작아지지만 일정 횟수 이상 반복할 경우 트레이닝 에러는 작아지지만 검증 에러는 커지는 **overfitting**(과적합)에 빠지게 된다. (모델이 범용적으로 좋은 방향으로 개선되는것이 아닌, 트레이닝만을 위한 방향으로 개선되는 것)
>
> 따라서 트레이닝 에러는 작아지지만 검증 에러는 커지는 지점에서 업데이트를 중지하면 최적의 파라미터를 얻을 수 있다.
>
> 이를 위해 검증용 데이터를 사용하는 것.
>
> - 오버피팅: 학습 과정에서 머신러닝 알고리즘의 파라미터가 트레이닝 데이터에 과도하게 최적화되어 트레이닝 데이터에 대해서는 잘 동작하지만 새로운 데이터인 테스트 데이터에 대해서는 잘 동작하지 못하는 현상.
> - 언더피팅(underfitting): 오버피팅의 반대 상황으로 모델의 표현력이 부족해서 트레이닝 데이터도 제대로 예측하지 못하는 현상
>
> 딥러닝의 경우 모델의 표현력이 강력하기 때문에 오버피팅에 빠지기 쉽기 때문에 오버피팅 문제를 완화하기 위해서 드롭아웃(dropout)과 같은 다양한 Regularization 기법을 사용한다.
>
> - Regularization 기법: 오버피팅을 방지하는 기법들을 총칭하는 단어



##### 데이터 보관 방법

- **CSV**(comma-separated values)

  몇 가지 필드를 쉼표(,)로 구분한 텍스트 데이터 및 텍스트 **파일**이다. 확장자는 `.csv`

- **SQLite**

  SQLite는 **파일기반의 임베디드 SQL 데이터베이스 엔진**이다.

  주로 응용 프로그램에 넣어 사용하는 비교적 가벼운 데이터베이스이다

  빠르고, 사용하기 쉽고, 무료의 경량 라이브러리이다.

---

## 2. 데이터 다루기

#### 0. 데이터 포함

예시 - csv파일

csv파일을 코드에서 가져와 사용하는 법

```python
import pandas as pd

df = pd.read_csv('D:\\Project\\tmp\\.venv\\heart_failure_clinical_records_dataset.csv')
```

#### 1. Data Processing

다운받은 데이터에서 필요한 데이터만 추려내는 과정

(주로 pandas 라이브러리 사용)

csv파일을 들여다본 후 판단해도 됨.

- **코드로 구현**

  1. 데이터를 수치형 데이터와 범주형 데이터로 나눈다.

  ```python
  # 수치형 데이터
  x_num = df[['age','creatinine_phosphokinase',    
         'ejection_fraction','platelets',     
         'serum_creatinine', 'serum_sodium']]
  
  # 범주형 데이터
  x_cat = df[['anaemia','diabetes', 'high_blood_pressure', 
          'sex', 'smoking']]
  
  # 결과 데이터
  y = df['DEATH_EVENT']
  ```

  >각 컬럼에 대해서 수치형 데이터, 범주형 데이터를 나눈다.
  >
  >df.info()를 통해 데이터가 수치형인지(int 또는 float) 범주형인지(object) 알 수 있다.



#### 2. Exploratory Data Analysis, EDA

탐색적 데이터 분석 작업, 데이터를 훑어본다고 생각하면 된다.

사용가능한 데이터인지, 원하는 분포를 가진 데이터인지 등 데이터의 특성을 찾는다는 목적을 가지고 있다.

(seaborn 라이브러리를 사용해서 여러 그래프를 그려내 확인한다.)

- 코드로 구현

  ```python
  import seaborn as sns
  # 원하는 그래프 사용
  ```

  > 라이브러리 사용 방법: [seaborn](https://github.com/HibernationNo1/TIL/blob/master/study_Machine_learning/python_module/seaborn.md) 



#### 3. Feature Engineering

Maching Learning이 데이터를 받을 때 잘 동작하게끔 Data를 가공하는 작업이다. (전처리, 표준화 과정)

(sklearn 또는 pandas라이브러를 사용)

- 표준화

  데이터를 비교하기 쉽게 만드는 방법을 데이터 표준화라고 한다. 데이터 형에 따라 표준화 방법이 달라진다. 

  - 수치 데이터 표준화

    키와 몸무게처럼 서로 단위가 다른 수치를 직접 비교하는건 의미가 없다. 이런 경우 둘다 평균이 0이고 표준편차가 1인 데이터로 바꾼 후 비교하면 두 데이터간의 상관관계를 발견할 수도 있다. 이 외에도 척도화와 벡터 정규화방식으로 데이터를 변환하여 사용할 수 있다. 이런 식으로 데이터를 표준화 해서 학습에 사용한다.

  - 카테고리 데이터 표준화

    카테고리 데이터 사이의 유사도 계산을 위해 원-핫 인코딩이라는 방식을 사용한다. 데이터를 1 또는 0으로 나타내는 것이다. 



- 코드로 구현(예시)

  1. 수치형 데이터를 전처리하고 입력 데이터 통합 예시

  ```python
  from sklearn.preprocessing import StandardScaler
   
  x_scaled = scaler.fit(x_num).transform(x_num)
  # 스케일러의 transform를 사용하면 x_scaled는 numpy형으로 바뀌게 된다.
  # 그렇기 때문에 데이터를 다시 DataFrame으로 바꿔준다. (pandas사용)
  x_scaled = pd.DataFrame(data=x_scaled, index = x_num.index, columns = x_num.columns)
  # index와 columns는 x_num의 것을 그대로 가져와준다.
  ```

  2. 데이터를  학습 데이터와 테스트 데이터로 분류

  ```python
  from sklearn.model_selection import train_test_split
  
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)
  ```

  > `test_size = 0.3` : 30%는 test data로, 70%는 training data로 사용됨
  >
  > `random_state` : test data와 training data를 나누는 seed (값이 크게 중요하지 않음)

  

#### 4. Maching Learning

데이터를 입력받아 학습한다.



#### 5. Feature Importances

학습된 모델로부터 데이터 특징의 중요도를 확인한다.

학습에 사용된 모델에 따라 특징의 중요도를 확인하는 방법은 각각 다르다.