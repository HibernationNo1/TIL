# 질문

## CS

### Laguage

- **OOP란 무엇인가?**

  Object Oriented Programming, 객체 지향 프로그래밍을 의미한다.

  현실 세계를 프로그래밍으로 옮겨와 현실 세계의 사물들을 객체로 보고, 그 객체로부터 개발하고자 하는 특징과 기능을 뽑아와 프로그래밍하는 기법이다. 

  캡슐화, 상속, 다형성 등과 같은 특징이 있다.



- **객체 지향 프로그래밍의 특징을 설명하시오**

  - 캡슐화 : 객체의 속성(data)과 행위(method)를 하나로 묶고 실제 구현 내용 일부를 외부에 감추어 은닉하는 것을 의미한다.
  - 다형성 :  같은 이름의 class나 function, method가 호출에 대해 객체에 따라 다른 동작을 할 수 있도록 구현하는 것을 의미한다.
  - 상속 : 상위 개체의 속성을 하위 개체에게 물려줌으로써, 하위 개체가 상위 개체의 속성을 모두 가지고 있음을 의미한다. 

  장점 : 캡슐화로 인해 유지보수에 용이하다. 코드의 재사용이 가능하다.

  단점 : 속도가 절차적 프로그래밍보다 상대적으로 느려지고 많은 양의 메모리를 사용하는 경향이 있다.

  대표 : java, python, C++



- **절차 지향 프로그래밍의 특징을 설명하시오**

  물이 위에서 아래로 흐르는 것처럼 순차적인 처리를 중요시하는 프로그래밍 기법이다.

  컴퓨터의 처리구조와 유사해 실행속도가 빠르지만, 코드의 순서가 바뀌면 동일한 결과를 보장하기 어렵다.

  대표 : C언어

  

- **Parameter와 Argument의 차이를 설명하시오**

  Parameter는 함수를 정의할 때 사용된 변수를 의미하고

  Argument는 함수를 호출할 때 parameter로 전달되는 실제 값을 의미한다.



- **Call By Value와 Call By Reference의 차이를 설명하시오**

  Call By Value는 인자로 받은 값을 copy해서 새로운 variable에 할당하는 방식으로, 원래의 값과 독립적이며 메모리 사용량이 늘어난다.

  Call By Reference는 인자로 받은 값의 주소를 참조하는 방식으로, 원래의 값을 의미하기 때문에 속도는 빠르지만 값에 영향을 주는 리스크가 존재한다.



- **프레임워크와 라이브러리 차이를 설명하시오**

  프레임워크와 라이브러리는 실행 흐름에 대한 제어 권한이 어디 있는지에 차이가 있다.

  프레임워크는 실행 흐름을 자체적으로 제어하지만 라이브러리는 흐름에 대한 제어 권한이 사용자에게 있다. 그렇기 때문에 라이브러리는 사용자가 주도성 있게 가져다 사용하는 것이고, 프레임워크는 사용자가 틀 안에 들어가서 사용한다는 관점으로 접근할 수 있다.
  
  프레임워코 : DjangGo  라이브러리 : tensorflow 



- **컴파일러(Compiler)란 무엇인가?** 

  한 언어에서 다른 언어로 번역하는 프로그램을 컴파일러라고 부른다. 번역 과정을 사전에 처리하여 빠르게 구동될 수 있도록 한다.

  주로 고급언어에서 기계어로 번역하는데 사용된다. 컴파일러를 실행시키면 언어를 모두 번역해서 하나의 바이너리(또는 어셈블리)파일로 저장한다. 

  장점 : 바이너리를 실행시키는 것이기 때문에 실행속도가 인터프리터에 비해 빠르다

  단점 : 컴퓨터마다 컴파일을 해 줘야 한다. 즉, OS 환경에 맞게 호환되는 빌드환경을 구분해서 구축해줘야 한다.(e.g, x86과 x64를 구분해서 다운)



- **Interpreter(인터프리터)**

  한 언어에서 다른 언어로 한 줄씩 번역하는 프로그램을 인터프리터라고 한다. 

  bash interpreter를 실행시키면 bash interpreter가 해당 파일의 첫번째 줄을 읽어 기계어 명령어로 번역 한 후 CPU에 바로 돌린다. 인터프리터가 번역해야하는 프로그래밍 언어를 스크립팅 언어라고 부른다.

  장점 : 인터프리터만 설치되어 있으면 같은 프로그램(스크립트)을 다른 컴퓨터에서도 실행 시킬 수 있다.

  단점 : 각 행마다 번역 및 실행을 거쳐야 한다. 따라서, 컴파일된 프로그램에 비해 속도가 느리다.



#### Python

- **파이썬의 주요 특징은?**

  객체 지향 언어로, 간결하고 단순하며 수많은 커뮤니티의 지원을 받고 있어 관련 정보를 얻기 쉽다.



- **map()은 어떤 역할을 하나?**

  iterable한 자료형의 data를 element wise로 함수에 전달인자로 사용하여 실행된 값을 리턴하는 함수이다. 

  map()의 첫 번째 전달인자는 함수고, 두 번째 전달인자는 iterable한 자료형(list, tuple)이다.



- **list append 와 list extend 의 차이점은 무엇인가?**

  append는 객체 자체를 해당 list의 마지막 index 뒤에 하나의 element로 붙이는 것이고, 

  extend는 다른 list의 element를 해당 list의 마지막 index 뒤에 순서대로 추가하는 것이다. 



- **.pyc 파일과 .py 파일과의 차이점은 무엇인가?**

  .pyc는 .py 파일의 컴파일된 버전이다. 플랫폼에 독립적인 바이트코드를 가지고 있어 .pyc 형식을 지원하는 모든 플랫폼에서 실행할 수 있다. Python은 성능을 향상시키기 위해 자동으로 .pyc 파일을 생성한다. .pyc 파일은 PVM(Python Virtual Machine)에 의해 실행되고, 지워도 되지만, 성능 저하가 생길 수 있다.



- **파이썬에서 main() 이 있는가?**

  C++언어의 main함수처럼 동작하는 main함수는 없다. 다만, 직접 파이썬 파일을 실행하는 경우에는 `__name__` 변수에 `__main__` 이라는 값이 할당되는데, 이를 이용해 `if`문을 사용하여 사용자가 정의한 main함수를 사용할 수 있다.  

  `if __name__ == "__main__"` 과 같이 조건문을 사용하면 터미널에서와 같이 직접 호출되어 사용될 때는 그 자체로 기능을 수행하고, 동시에 다른 모듈에서 필요한 함수 등을 제공할 수 있다.

  

- **파이썬 메모리는 어떻게 관리되는가?**

  파이썬에서는 개별적인 힙을 사용해서 메모리를 유지한다. 힙에는 모든 파이썬 객체와 자료구조를 가지고 있다. 이 영역은 파이썬 인터프리터만이 접근 가능하며 프로그래머는 사용할 수 없다.



- **list와 tuple의 차이는?**

  둘 다 collection 데이터 타입의 자료형으로, list는 mutable(변경 가능) 하고 튜플은 immutable(변경 불가능)하다.



- **Docstring은 무엇인가?**

  Docstring은 파이썬의 module, function, class, method 정의의 처음 부분에 선언되는 유니크한 텍스트로, 코드의 문서화에 도움이 되는 문자열을 말한다.

  쌍따옴표 세개를 사용하여 작성한다. 

  `help()` 함수를 사용하여 docstring 내용을 확인할 수 있다.



- **Erros와 Exceptions의 차이는 무엇인가?**

  Erros는 coding issue로 인해 프로그램이 비정상적으로 종료되는 것이고

  Exceptions는 외부적인 event로 인해 프로그램이 종료되는 것이다. (load하려는 file이 없을 때)



- **decorator란 무엇인가?**

  함수를 수정하지 않은 상태에서 추가 기능을 구현할 때 사용하는 기능으로, `@함수명`으로 함수를 호출해서 추가 기능을 붙여넣어 사용한다.



- **enumerate이란 무엇인가?**

  순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스 값을 포함하는 enumerate 객체를 리턴한다.



- **With, as는 왜 사용가는가?** 

  파이썬에서는 파일을 다루는 처리를 할때는 필수적으로 파일 오픈(open) 파일 닫기(close) 과정을 거치게 되고, 코드가 복잡해지면서 개발자는 파일을 열어놓고 닫지 않는 실수를 할 경우가 생길때도 있다. 

  with ... as 구문을 사용하게 되면 파일을 열고 해당 구문이 끝나면 자동으로 닫히게 되어서 이러한 실수를 줄일 수 있다

  with를 통해 API를 열어 GradientTape 기록에 사용할 수 있다.



- **pass와 continue의 차이는?**

  파이썬은 함수등을 작성할때 반드시 무언가를 써야 문법적으로 올바르기 때문에, pass를 사용하여 문법 오류를 막을 수 있다. continue는 단순히 반복문에서 해당 반복문을 해당 위치에서 더이상 진행하지 말고 다음 iteration으로 넘어가게 하는 것이다.



- **Lambda와 Def의 차이점은?**

  lambda함수는 단일 함수로, 함수 객체를 구성하고 바로 반환한다.

  def는 다중 표현을 갖는 함수로, 함수를 생성하고 이름을 지정하여 호출이 가능하다. def는 선언을 반환할 수 있지만 lambda는 불가능하다.



- **오버로딩이란 무엇인가?**

  동일한 이름의 함수를 매개변수에 따라 다른 기능으로 동작하도록 하는 것이다. 파이썬에서는 연산자 오버로딩을 지원하는데, 이는 연산자를 객체끼리 사용할 수 있게 해준다.



##### Class

- **Class란 무엇인가?**

  객체를 만들어내기 위한 설계도로, 객체에 대한 data와 method(기능)을 묶어 놓은 틀이다.



- **Object란 무엇인가?**

  class를 기반으로 선언된 대상이다. 



- **Instance란 무엇인가?**

  Object에 메모리가 할당되어 실제로 활용되는 실체이다.



- **파이썬 클래스에서 Attribute는 무엇인가?**

  클래스 내부에 포함돼 있는 methods나 variables을 의미한다. Class Attribute, Instance Attribute 두 가지가 있다.

  - **Class Attribute** : method 계층과 동일한 영역에 위치한 변수를 의미한다. 

    Class Attribute에 접근할 경우, 모든 클래스에 동일하게 영향을 미친다.

  - **Instance Attribute**  : Class를 호출해서 만들어진 Instance만의 변수를 의미한다. 

    `__init__` method에서`self.` 를 사용해 정의한다. Instance를 초기화 할 때 생성된다.

  

- **상속이란 무엇인가?**

  객체가 부모 클래스의 특징에 접근할 수 있는 것을 의미한다.



- **Overriding(오버라이딩)이란 무엇인가?**

  부모 Class에서 정의한 method를 자식 class에서 재정의하는 것으로, 부모 class의 method 이름과 기본적인 기능은 그대로 사용하지만, 특정 기능을 바꿀 때 사용한다.

  





## Artificial Intelligence

- **인공지능이란?** 

  기계가 주어진 문제를 합리적인 사고를 통해 해결하는 인지적인 능력을 갖추고 있고, 이러한 문제를 해결하는 경험을 통해 능력을 향상하는 학습 능력을 갖추도록 하는 기술이다.



- **Machine Learning이란?**

  데이터를 통해 학습하며 패턴을 식별하며, 학습된 정보를 바탕으로 합리적인 결정을 도출해내는 인공지능의 기법 중 하나이다.



- **Deep Learning이란?**

  딥러닝이란 심층 신경망을 사용해서 기계를 학습시키는 알고리즘을 뜻한다. 여러 머신러닝 기법 중 하나다.

  - 머신러닝과의 차이점

    머신 러닝 알고리즘은 모델이 부정확한 예측을 반환하면 엔지니어가 개입하여 조정해야 한다.

    딥러닝 알고리즘은 자체 신경망을 통해 스스로 판단해서 학습한다.



- **data mining이란?**

  대규모로 저장된 데이터 안에서 체계적으로 특정 규칙이나 패턴을 분석하여 가치있는 정보를 추출하는 과정

  - 머신러닝과의 차이

    데이터 마이닝은 가지고 있는 데이터에서 현상 및 특성을 발견하는 것이 목적이다.

    머신러닝은 기존 데이터를 통해 학습을 시킨 후 새로운 데이터에 대한 예측값을 알아내는 것이 목적이다.



- **Big data란?**

  빅 데이터는 대량의 데이터셋(빅 데이터)을 수집하고 분석하는 기술이다.

  - 머신러닝과의 차이

    빅 데이터는 대용량 데이터에서 유용한 패턴을 발견하는 것이 목적이다.

    머신러닝은 기존 데이터를 통해 학습을 시킨 후 새로운 데이터에 대한 예측값을 알아내는 것이 목적이다.



## Machine Learning

- **머신러닝 학습 종류**

  - 귀납적 머신러닝 

    통계적 학습을 통해 기계학습을 하는 방법으로, 표본으로부터 일반화하는 학습 방법이다

  - 연역적 머신러닝

    논리적 학습을 통해 기계학습을 하는 방법. 사실 이 부분에 대해서는 가능한가 싶다. (삼각형의 내각-기하학 원리)



- **머신러닝 학습 방법 세 가지에 대해서 설명하시오**

  - **Supervised Learning**(지도 학습 )

    정답에 해당하는 Label 데이터를 통해 학습을 하는 방법이다.  (분류, 회귀 모델에 사용된다.)

  - **Unsupervised Learning**(비지도 학습 )

    정답을 따로 알려주지 않는 학습 방법이다. 정답이 없기 때문에 Input Data를 기반으로 군집을 찾는 학습을 진행한다.

  - **Reinforcement Learning**(강화 학습)

    주변 상태에 따라 어떤 행동을 할지 판단을 내리는 주체인 에이전트가 있고, 에이전트가 속한 환경이 있다. 에이전트의 판단에 의해 에이전트의 상태가 바뀌게 되고, 보상을 받을 수도 있다. 강화학습의 목표는 주어진 환경에서 보상을 최대한 많이 받을 수 있는 에이전트를 학습하는 것.



- **classification에 대해서 설명하시오**

  범주형 데이터의 패턴을 분석해 input data가 어떤 범주에 속하는지를 예측하는 알고리즘



- **Regression에 대해서 설명하시오**

  데이터의 연속된 패턴을 분석해 다음 output에 어떤 값을 출력할지를 예측하는 알고리즘



- **clustering에 대해서 설명하시오**

  정답이 따로 분류되어있지 않은 데이터 안에서 패턴과 구조를 분석해 비슷한 속성의 데이터끼리 묶어주는 알고리즘



- **분류 모델의 성능 평가 지표에 대해서 설명하시오**

  > - True Positive(TP) : 정답인데 정답이 맞는 경우
  > - False Positive(FP) : 정답이 아닌데 정답이라고 한 경우(오류)
  > - False Negative(FN) : 정답인데 정답이 아니라고 한 경우(오류)
  > - True Negative(TN) : 정답이 아닌데 정답이 아니라고 한 경우

  - **Accuracy(정확도)**

    직관적으로 모델의 성능을 나타낼 수 있는 평가 지표.

    전체 예측 건수에서 정답을 맞힌 건수의 비율

    단점 : 정확도 역설(Accuracy Paradox)  

    실제 데이터에 Negative 비율이 너무 높아서 희박한 가능성으로 발생할 상황에 대해서는 제대로 된 분류가 되지 않는다. ex) 눈이 1미터 이상 쌓이는 경우

  - **Precision(정밀도)**

    모델이 True라고 분류한 것 중에서 실제 True인 것의 비율

    단점: False Positive(FP) 가 존재해도 정밀도가 100%가 나올 수 있다.

  - **Recall(재현율)**

    실제 True인 것 중에서 모델이 True라고 예측한 것의 비율

    단점: 언제나 Ture만 답하는 분류기에서는 Recall이 1이 된다.

  - **F1 score**

    Precision과 Recall의 조화평균이다.
    
    데이터 label이 불균형 구조일 때, 모델의 성능을 정확하게 평가할 수 있으며, 성능을 하나의 숫자로 표현할 수 있습니다.



- **회귀 모델의 성능 평가 지표에 대해서 설명하시오**

  loss function이 곧 회귀 모델의 성능 평가 지표가 된다.



- **하이퍼 파라미터와 딥러닝 모델의 파라미터의 차이점을 설명하시오**

  | 비교 항목 | 하이퍼 파라미터                                       | 딥러닝 모델의 파라미터                                       |
  | --------- | ----------------------------------------------------- | ------------------------------------------------------------ |
  | 사용 목적 | 모델링 최적화 파라미터값 도출                         | 최적화된 딥러닝 모델 구현                                    |
  | 생성 주체 | 사용자 판단 기반 생성                                 | 데이터 학습 모델이 생성                                      |
  | 조정 여부 | 조정 가능                                             | 임의 조정 불가                                               |
  | 활용 사례 | – 학습률<br/>– 경사하강법 반복 횟수<br/>– 활성화 함수 | – 인공신경망의 가중치<br/>– SVM의 Support Vector<br/>– 선형회귀에서의 결정계수 |

  하이퍼파라미터는 데이터 분석 결과로 얻어지는 값이 아니므로 절대적인 최적값은 존재하지 않음.



- **Data normalization을 하는 이유에 대해서 설명하시오**

  normalization : 사용하는 데이터의 scale 범위를 줄여준다.

  사용하는 데이터의 scale 범위가 크면 노이즈가 생성되기 쉽다. 이런 경우 모델은 어느 feature가 더 중요하고 덜 중요한지 알기 힘들기 때문에 Overfitting이 일어나기 쉽기 때문이다. 



- **standardization을 하는 이유에 대해서 설명하시오**

  standardization : 데이터의 평균을 0, 표준편차를 1로 보정하는 것.

  가장 이상적인 cost function을 구하기 위해 0에 대칭적이고 양, 음의 평균 절대값이 1인 data set을 구현하는 것이다. 

  이러한 data set이 바로 **standard normal distribution**(표준 정규분포)이다.



## Deep-learning

기계가 데이터를 받아 심층 신경망을 통해 학습하는 경우를 **딥러닝(Deep Learning)**이라고 한다.

- **perceptron(neural)**

  실제 뇌의 신경세포 뉴런의 동작을 모방한 알고리즘으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘이다. 각각의 입력값에는 각각의 가중치가 존재하고, 가중치가 클수록 해당 입력 값이 중요하다는 것을 의미한다.

  - threshold : 퍼셉트론이 활성화 되기 위한 최소값



- **Single-layer Perceptron**

  여러 퍼셉트론이 하나의 층을 이루고 있는 구조로 입력 벡터를 두 부류로 구분하는 선형분류기이다. 이를 통해 AND, NAND, OR게이트를 구현할 수 있다.



- **Multi-layer Perceptron**

  입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 둔 구조이다. 

  AND, NAND, OR게이트를 조합하여 XOR게이트를 구현하는 것 처럼, 비선형 분류를 구현하기 위한 구조이다.

  다층 퍼셉트론을 통해 기계 학습을 진행하는 기술을 deep learning이라고 한다.



- **딥러닝에서 말하는 학습의 대상이란?** 

  Model을 setting하게 되면 Model Parameters값을 얻게 되는데, 이 weight, bias와 같은 parameter가 바로 deep learning에서 말하는 target of learning이다.

  이러한 parameter값을 여러 learning을 거치며 정답에 가까운 값으로 예측하게 하는 것이 deeplearning의 목적이다. 이를 **Function approximation**이라고 한다.

  - weight : 선형 경계의 방향성 또는 형태를 나타내는 값, 입력되는 각 신호가 결과 출력에 미치는 중요도를 조절하는 매개변수

  - bias : 뉴런의 활성화 함수의 사용 여부를 조절하는 매개변수

    Wx+b 에서 b가 큰 양수면 활성화되기 쉬우나(즉 1이 되기 쉬우나), 큰 음수면 활성화되기 어렵(즉 0이 되기 쉽다)기 때문에 b를 통해 활성화 함수의 사용 여부를 조절할 수 있다.

    

- **Back Propagation**

  Multi-layer Perceptron에서 output에서 input방향으로 진행하며 gradient를 chain rule에 의해 전달해준다. gradient는 gradient descent method의 식으로 인해 Weight를 재업데이트 하는데 사용된다.

  Oupput Layer의 Target Value가 있는 것과 달리 중간의 Hidden Layer가 가지고 있는 node에는 Target Value가 존재하지 않아 오차(목표값과 실측값의 차이)를 줄이기 위해서 사용하는 방법이다. 

  - chain rule: 오차를 줄이기 위해 가중치를 체계적으로 변경하는 방법

    입력 노드 A가 출력 노드 B의 오차에 기여했다면, 두 노드의 연결 가중치는 A노드의 출력과 B노드의 오차에 비례해 조절한다.

  
  
- **Overfitting**

  Hidden Layer를 늘림으로 인해 기계가 training data에 의해 학습이 training data에만 과최적화되어(noise까지 학습하게 되어) 실제 data에 의한 정확도가 낮아지는 현상이다.

  해결법 

  - Drop out : 학습 과정에서 일부 뉴런을 사용하지 않는 형태로 만들어서 오버피팅을 방지한다.
  - Cross validation : training data set에서 일부를 validation data set으로 사용해서 평가에 사용되는 데이터 편중을 막을 수 있고, 정확도를 향상시킬 수 있다.



- **Regularization**

  OverFitting을 방지할 수 있도록 만들어주는 기법들을 총칭해서 Regularization라고 한다.

  Large Dataset, Drop Out, Cross validation, L1, L2 Regularization과 같은 방법이 있다.



- **Vanishing Gradient Problem**

  Hidden Layer의 수가 많아지면, 역전파가 진행되는 동안 chain rule에 의해 오차가 0인 뉴런이 발생할 확률이 높아지고, 이로 인해 loss function의 gradient가 0이 되어 학습이 되지 않는 현상.

  - 해결방법 : 적절한 활성화 함수를 사용한다.



- **One-hot Encoding**

  데이터를 수많은 0과 한개의 1의 값을 가진 벡터의 차원으로 변형하는 기법으로, 컴퓨터에게 각 class에 대한 title을 알려주는 방법이다. 



- **Batch Normalization을 하는 이유에 대해서 설명하시오**

  Batch Normalization(배치 정규화) : 데이터의 평균과 분산을 조정하는 과정(주로 신경망 안에 포함되어 학습 과정에서 이루어진다.)

  - 이유 1: 각 layer를 통과 할 때마다 input 데이터의 분포가 조금씩 변하는 현상인 **Covariate Shift**를 방지하기 위해서이다.

    Covariate Shift가 발생하는 이유 : 학습 중간에 가중치가 계속 업데이트 되면서 학습 데이터의 분포가 수시로 바뀌게 된다.

    

- **Weight Initialization(가중치 초기화)를 하는 이유**

  1. 값이 균일하거나 대칭적이면 안된다.

     왜? 학습 시 모든 노드의 활성화 함수에 같은, 또는 대칭적인 입력값이 주어지게 되고, 역전파가 진행될 때 모든 가중치의 값이 똑같이 갱신되기 때문이다. 이는  한 layer의 노드들이 모두 같은 일을 하기 때문에 노드의 낭비로 이어진다.

  2. 어떤 활성화 함수를 사용하느냐에 따라서 달라져야 한다.

     - sigmoid, tanh 를 사용할 때 : weight를 큰 값으로 주면 함수의 gradient 값이 0에 가까워져 Gradient Vanishing 문제 발생
     - ReLU를 사용할 때 : weight를 작은 값으로 주면 Dying ReLU 현상 때문에 가중치 갱신이 일어나지 않는다.

  - sigmoid, tanh 함수를 사용할 때 : Xavier Initialization 사용

    input, output leyer의 unit 개수를 감안하여 weight Initialization을 하는 기법
    $$
    std = \sqrt{\frac{2}{fan\ in + fan \ out}}
    $$
    unit이 많아질수록 표준편차는 줄어드므로, 앞 층에 노드가 많을수록 대상 노드의 초기값으로 설정하는 가중치가 좁게 퍼진다. (Gradient Vanishing 문제 예방)

  - ReLU 함수를 사용할 때 : He Initialization 사용

    input leyer의 unit 개수만 감안하여 weight Initialization을 하는 기법
    $$
    std = \sqrt{\frac{2}{fan\ in }}
    $$
    ReLU의 양의 값을 더 넓게 분포시키며 Dying ReLU 현상을 예방할 수 있다.

  

- **Linearity와 Non-Linearity의 차이는?**

  동차성 : f(ax) = a × f(x) 

  가산성 : f(x1 + x2) = f(x1) + f(x2) 

  두 가지 조건이 성립해야만 Linearity 이라고 할 수 있다.

  - 왜 Non-Linearity이 사용되나?

    분류 모델을 만든다고 했을 때 구분선이 선형이 되지 않는 경우가 훨씬 많다. 이를 위해 비선형성을 부여하고, 그로써 더욱 정규한 구분선을 찾도록 훈련한다.

    선형 시스템이 아무리 깊어지더라도 가산성의 성질에 의해 하나의 linear 연산으로 나타낼 수 있기 때문이다.



### Activation Function

neural의 출력을 다음 neural으로 전달할지 여부를 결정하는 역할을 한다. 이를 통해 퍼셉트론의 입력값에 대한 출력값이 linear하게 나오지 않게 결정해서 비선형 시스템을 만들 수 있다.

- **Sigmoid** : output값을 0에서 1사이로 만들어준다. 데이터의 평균은 0.5를 갖게된다.
  $$
  \sigma(x) = \frac{1}{1+e^{-x}}
  $$
  ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbDk83K%2FbtqAZO51QIQ%2FIZrbpIaB8qwnnBFtX7M7IK%2Fimg.png)

  Sigmoid의 도함수를 살펴보면 최대값이 0.25인 것을 알 수 있다. 망이 깊어질 수록 Gradient가 1/4씩 줄어든다는 의미이다.
  
- **Tanh** : output값을 -1에서 1사이로 만들어준다. 데이터의 평균은 0를 갖게된다.
  $$
  \frac{e^{z} -e^{(-z)}}{e^{(z)} + e^{(-z)}}
  $$
  ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc0ambs%2FbtqA2VvQ82W%2FaUsDa9VwaMcKiOfR1UZqJ0%2Fimg.png)

- **ReLU** : 0보다 큰 데이터는 그대로 출력한다. 대부분의 input값에 대해 기울기가 0이 아니기 때문에 학습이 빨리 된다. 
  $$
  ReLU(z) = \left\{\begin{matrix}
  0,\ \ \ \ where\ z <0
  \\ 
  z,\ \ \ \ where\ z \geq 0
  \end{matrix}\right.\\ max(0, z)
  $$
  ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcfAwRD%2FbtqA1y2vSnH%2FeL8PnTymMANrq5TfV7BNw0%2Fimg.png)

  - 대부분의 input값에 대해 기울기가 0이 아니기 때문에 Vanishing Gradient 문제가 발생하지 않는다.
  - 문제점 : Forward Propagation과정에서 음수 값이 입력되면 BackPropagation 시 0의 값이 가중치에 곱해지면서 neural이 활성화 되지 않는다는 문제점이 있다. (Dying ReLU)
  
- softmax : output값을 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다.
  $$
  softmax(x)_i = \frac{exp(x_i)}{\sum_jexp(x_j)}
  $$
  분류기에서 classicification을 위해 output layer에 사용된다.



### Loss Function

파라미터값이 목적에 적합한 값인지 측정하는 함수를 loss function이라고 한다.

- **cost function**

  특정 batch size에 대해서 loss function의 평균을 의미한다.



- **Mean Squared Error**

  추측값에 대한 정확성 측정 방법으로, 오차의 제곱에 대해 평균을 취한 것이다. 



- **Categorical Cross Entropy**

  주로 여러개의 class로 classification 할 때 사용하는 Loss Function이다. 오직 실제 정답과의 오차만을 파악한다. label 데이터가 one-hot 형태로 제공되야 한다.



### Tensorflow

- **keras의 주요 특징을 설명하시오**

  - 모듈화 (Modularity) : 케라스에서 제공하는 모듈은 독립적이고 설정 가능하며, 가능한 최소한의 제약사항으로 서로 연결될 수 있다.

  - 최소주의 (Minimalism) : 각 모듈은 짥고 간결하다.
  - 쉬운 확장성 : 새로운 클래스나 함수로 모듈을 아주 쉽게 추가할 수 있다. 따라서 고급 연구에 필요한 다양한 표현을 할 수 있다.
  - 유저가 손쉽게 딥 러닝을 구현할 수 있도록 도와주는 파이썬 딥러닝 라이브러리이다.



- **tensors의 종류에 대해서 설명하시오**

  tensors에는 두 개의 클래스가 있다.

  값을 담으면 immutable한 tensor인 **EagerTensor**와 값을 담아도 mutable한 tensor인**ResourceVariable**가 있다. 학습으로 인해 업데이트가 필요한 파라미터는 ResourceVariable로 사용한다.



- **GradientTape은 무엇인가?**

  주어진 입력 변수에 대한 연산의 편미분(gradient)를 계산하는 API로, 

  컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 "기록" 후 후진 방식 자동 미분(reverse mode differentiation)을 사용해 테이프에 "기록된" 연산의 그래디언트를 계산한다.

  쉽게 말자하면 forward propagetion 과정을 기록해 둿다가, back propagetion을 진행할 때 사용하기 위한 API다.

  - **gradient()**

    gradient 함수는 인자로 가져온 값에 대한 편미분을 계산한다.

    GradientTape()에 의한 스코프 안의 변수를 인자로 가져오기 때문에 `스코프 이름.gradient` 형태이다.

  - **apply_gradients()**

    gradient 값을 통하여 theta 값을 updata하기위한 그래프를 생성하는 함수다. 최적화 도구에 쓰이기 때문에 `optimizer.apply_gradients` 형태로 사용되며 grdient descent가 진행돼 parameter updata가 이루어지게 된다.



- **모델 구현 방식에 대해서 설명하시오**

  -  **Sequential API** 사용 : 네트워크의 입력과 출력이 하나라고 가정하고 단순하게 layer를 쌓는 방식으로 구현하는 방식이다.

    쉽고 사용하기가 간단하지만 다수의 입-출력을 가진 모델 또는 층 간의 연결을 구현하기에는 적합하지 않다.

  - **Functional API** 사용 : 네트워크가 directed acyclic graph(비순환 그래프)라고 가정하고 model과 layer를 일종의 function으로 정의하여 model을 구현하는 방식이다.

    각각의 변수에 layer를 받아 모듈별로 구성할 수 있으며, 마지막에는 Model()객체에 input과 output텐서를 지정하여 모델을 생성한다.

    다중입력, 다중출력 model을 구현할 때 사용되는 방식이다.

    재귀 네트워크나 트리 RNN model은 DAG가 아니기 때문에 Functional API 방식으로 구현할 수 없다.

  - **Subclassing API** 사용 : keras model을 상속받는 class를 생성 후 그 안에서 네트워크의 조건에 상관 없이 밑바닥부터 새로운 수준의 아키텍처를 구현하는 방식이다.

    알고리즘의 디테일한 부분을 컨트롤할 수 있지만, 복잡한 코드구조를 가지고 있다.

    



- **Tensorboard란 무엇인가?**

  학습 과정을 시각화해서 확인할 수 있도록 도와주는 기능이다.

  학습 과정 실시간 확인, 기존 학습과 비교가 가능하다.

  오류 없이 tensorboard를 실행하면 브라우저로 6006번 포트에 접근해 확인할 수 있다.



### Optimizer

손실함수를 최소화하는 방향으로 파라미터들을 업데이트하는 학습 알고리즘을 의미한다.



- **Gradient Descent** : 제시된 loss function의 1차 미분계수를 이용해 loss function의 최소값을 찾아가는 알고리즘이다.

  주로 Stochastic Gradient Descent (SGD)을 사용한다. (Stochastic : 확률적, 랜덤하게 추출한 일부 데이터를 Mini-Batch 묶어 사용한다는 의미) 
  
  - 왜 꼭 gradient를 써야 할까?
  
    gradient는 함수가 가장 빠르게 증가하는 방향의 벡터를 의미하기 때문에, loss function의 gradient가 0인 지점을 찾아가는 방법은 loss function이 최솟값을 갖는 위치를 찾아가는 방법으로 사용될 수 있는 것이다.
  
    (눈 감고 산 내려가기)



- **Momentum**

  **overshooting** 을 방지해서 SGD보다 빠른학습속도를 얻고, local minimum를 문제를 개선하고자 SGD에 관성의 개념을 적용한 최적화 기법이다. 

  과거에 parameter가 update했던 방식을 누적, 참고하여 update 진행 방향에 가속도를 붙여주어 빠르게 local minimum을 벗어나고, global minimum에 도달하기 위한 기법이다. 
  
  - local minimum(국소값) : 함수의 최소값이 아닌, 단순 기울기가 0이 되는 지점
  - overshooting : gradient값이 너무 크거가 너무 작아서 loss function의 최저점을 향한 수렴이 잘 안 되는 현상을 뜻한다.



- **Nesterov Accelrated Gradient,NAG**

  최적의 parameter를 관성에 의해 지나칠 수 있는 Momentum의 문제점을 해결하기 위한 기법이다.  기존의 Momentum에서 기울기를 적용할 땐 업데이트 된 파라미터의 SGD를 사용했지만, NAG는 관성에 의해 이동된 곳에서의 파라미터의SGD 적용한다.



- **Adagrad**

  동일 기준으로 update되던 각각의 parameter에 개별 기준을 적용하는 방법이다. 

  지속적으로 변화하던 parameter는 최적값에 가까워졌을것이고 한 번도 변하지 않은 parameter는 더 큰 변화를 줘야한다는 것이 Adagrad의 개념이다.
  
  학습이 진행됨에 따라 변화 폭이 눈에 띄게 줄어들어 결국 움직이지 않게 되는 문제점이 있다.



- **RMSProp**

  학습이 진행됨에 따라 변화 폭이 눈에 띄게 줄어들어 결국 움직이지 않게 되는 Adagrad의 문제점을 해결하기 위한 기법으로, Adagrad의 계산식에 지수이동평균을 적용해서 학습에 필요한 최소 step은 유지할 수 있게 했다.

  - 지수이동평균: 최근 값을 더 잘 반영하기 위해 최근 값과 이전 값에 각각 가중치를 주어 계산한는 방법



- **Adaptive Moment Estimation (Adam)**

  RMSProp와 Momentum 기법을 합친 optimizer로, SGD에 관성의 개념을 적용하고 이후 지수이동평균을 적용하는 기법이다.



### Neural Network

#### Convolution neural network, CNN

Convolution이라는 전처리 작업이 들어가는 Neural Network 모델로, 주로 이미지나 영상 데이터를 처리할 때 쓰인다.

일반 ANN은 기본적으로 1차원 형태의 데이터를 사용하기 때문에 (예를들면 1028x1028같은 2차원 형태의)이미지가 입력값이 되는 경우, 이것을 flatten시켜서 한줄 데이터로 만들어야 하는데 이 과정에서 이미지의 공간적/지역적 정보(spatial/topological information)가 손실된다. 또한 추상화과정 없이 바로 연산과정으로 넘어가 버리기 때문에 학습시간과 능률의 효율성이 저하된다. 이러한 문제점에서부터 고안한 해결책이 CNN이다. CNN은 이미지를 날것(raw input) 그대로 받음으로써 공간적/지역적 정보를 유지한 채 특성(feature)들의 계층을 빌드업한다.

**장점**: 데이터의 지역정보를 효율적으로 볼 수 있다.

**단점**: 멀리 있는 픽셀 간의 관계를 보려면 layer를 많이 쌓아야 한다. (이미지가 크면 신경망이 깊어야 한다.)



- **Feature Extractor, CNN**

  image에서 특정 features를 추려내서 여러 Class로 분류한다. 

  fearture extractor 과정에서 Convolution과 Pooling Operation이 진행된다.



- **Fully connected, ANN(MLP)**

  Feature Extractor을 통과하고 나온 data를 input으로 받는다. 특정 Activation Function을 적용함으로써 output을 각각의 class에 대한 probability로 만들어준다.



- **Convolution Layer**

  특정 가중치가 곱해진 kernel을 이용해 합성곱을 수행하여 출력 노드에 입력함으로써 이미지의 특징을 추출해내는 역할을 한다. window slicing을 통해 반복한다.

  kernel을 통해 추출한 이미지에 Zero Padding 을 적용해 **활성화 맵(feature map)**을 만든다.  feature map은 원본 이미지에서 명확히 들어나지 않았던 특징들을 보여준다.   

  image의 영역 중 kerenel과 convolution 되는 영역을 **Receptive Field** 라고 한다.



- **Zero Padding**

  커널을 통해 추출한 이미지는 기존 이미지보다 크기가 줄어들기 때문에 손실되는 부분이 발생하는데, 이를 방지해서 이미지의 크기를 맞추기 위해 위해 추출한 이미지의 가장자리에 0으로 구성된 데이터 n겹을 감싼다. 이미지의 크기를 기존 이미지와 똑같이 맞춘다.



- **Poolng Layer**

  이미지의 차원을 축소함으로써 필요한 연산량을 감소시킬 수 있고, 파라미터가 줄어드니까 덩달아 drop out으로 인해 오버피팅 방지 효과도 따라온다.

  이미지의 가장 강한 특징만을 추출하는 특징 선별의 역할을 한다.

  최대값 풀링, 평균값 풀링, 최소값 풀링이 있다.

  - **Max Pooling** : feature maps에서 큰 값을 출력 노드에 입력하는 동작. 

    가장 큰 feature 값이 계산 시 가장 큰 영향을 주기 때문에 출력 값에 영향이 가장 크고 feature 즉, 특징을 가장 잘 나타내기 때문에 사용한다.

  - **Average Pooling** :  feature map의 평균값을 구해서 출력 노드에 입력하는 동작. 

    장점 : feature map 안의 값들의 평균을 사용하기 때문에 global context 정보를 가진다.

    단점 : Tanh activation function을 사용할 때 그 값이 서로 상쇄되거나, Relu activation function을 사용할 때 0이라는 value 때문에 activation average가 상쇄되는 문제가 발생할 수 있다.



- **Global Average Pooling(GAP)** : 같은 채널 (같은 색)의 feature들을 모두 평균을 낸 다음에 채널의 갯수(색의 갯수) 만큼의 원소를 가지는 벡터로 만드는 동작을 수행한다.

  GAP의 목적은 feature를 1차원 벡터로 만들기 위함이다.

  왜? CNN + FC(Fully Connected) Layer에서 classifier인 FC Layer를 없애기 위해.

  FC layer를 classifier로 사용하는 경우 파라미터의 수가 많이 증가하는 단점이 있으며 feature 전체를 matrix 연산하기 때문에 위치에 대한 정보도 사라지게 된다.

  반면 GAP는 어떤 크기의 feature 라도 같은 채널의 값들을 하나의 평균 값으로 대체하기 때문에 벡터가 된다. 따라서 **어떤 사이즈의 입력이 들어와도 상관이 없다.** 또한 단순히 (H, W, C) → (1, 1, C) 크기로 줄어드는 연산이므로 파라미터가 추가되지 않으므로 학습 측면에서도 유리하고, 파라미터의 갯수가 FC Layer 만큼 폭발적으로 증가하지 않아서 over fitting 측면에서도 유리하다.  따라서 GAP 연산 결과 1차원 벡터가 되기 때문에 **최종 출력에 FC Layer 대신 사용**할 수 있다.



- **Localization이란?**

  모델이 주어진 이미지 안의 Object 가 이미지 안의 어느 위치에 있는지 위치 정보를 출력해주는 것으로, 주로 Bounding box 를 많이 사용한다.



- **Object Detection이란?**

  객체 검출이라고 하는데, Deep Learning에서는 Classification 과 Localization 이 동시에 수행되는 것을 의미한다. YOLO는 Multi object detection이다.



#### Recurrent Neural Network, RNN

Sequence data와 같이 시계열 데이터를 다루기에 최적화된 인공신경망이다.

RNN은 기본적인 ANN구조에서 이전 시간(t-1)의 은닉층의 출력값을 다음 시간(t)에 은닉층의 입력값으로 다시 집어넣는 경로가 추가된 형태이다.

**장점**: 이전 상태에 대한 정보를 일종의 메모리 형태로 저장할 수 있어 시계열 데이터를 다룰 때 강력하다.

**단점**: Vanishing Gradient Problem이 발생할 수 있다. 시간 축 1에서 받은 영향력은 크게 남아있지만, 새로운 시간축에서 들어온 input데이터의 영향력에 의해 영향력이 반복해서 덮어씌워며 점점 약해진다.



- **Long Short-Term Memory, LSTM**

  LSTM은 은닉층을 각각의 노드가 input gate, forget  gate, output gate로 구성된 메모리 블럭이라는 조금 복잡한 구조로 대체한다. 



- **Embedding**

  자연어 처리 문제를 다룰 때 널리 사용되는 기법으로, Sparse한 One-hot Encoding의 데이터 표현을 Dense한 표편형태로 변환하는 기법이다.

  > one-hot Encoding은 데이터의 표현 형태가 Sparse(희소)하다는 문제점
  >
  > 데이터를 표현 할 때 의미있는 데이터는 전체 데이터 중 극히 일부분이다. 또한 one-hot  Encoding은 유사한 의미를 가진 단어간의 연관성도 표현할 수 없다.

  값이 채워져 있는 임베딩 매트릭스를 만든 후 Sparse한 데이터에 곱하면 더 작은 차원의 빽빽한 데이터로 변환된다. 임베딩 매트릭스가 잘 학습된, 의미있는 매트릭스일때  해당 매트릭스와 곱해서 계산된 매트릭스가 유의미한 데이터의 연관성을 표현할 수 있다. 



## Computer Vision



- **Region of interest, ROI**

  관심 영역을 뜻한다. 영상에서 특정 연산을 수행하고자 하는 임의의 부분 영역을 의미한다.

  ROI영역에 대해 표현된 image를 mask영상이라고 한다.

